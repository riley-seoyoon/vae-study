{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"\"  # Set the GPU\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24571316,  1.20752331, -1.34911164, -1.08253465,  1.49166842,\n",
       "         0.20202308, -0.65336259, -0.13298715,  0.96503092, -1.55293706],\n",
       "       [ 1.0871331 ,  0.44521698, -0.98670451, -2.23006552, -0.41548319,\n",
       "         0.25337438,  0.44956698, -0.86520142, -0.0547919 ,  0.79194314],\n",
       "       [ 1.97585154,  1.08755116, -0.87196726, -0.53131319,  0.75324479,\n",
       "         0.32084094, -2.35121855, -0.74390211,  0.64182779, -1.85149413],\n",
       "       [ 2.01916171,  0.65104069, -0.71712066, -0.95691518,  1.98095011,\n",
       "         0.43648931, -0.7118163 , -0.60416282, -1.14906209, -0.23826607],\n",
       "       [ 1.35141038,  1.3247779 , -0.22267992, -0.79041832,  2.69019775,\n",
       "         0.49025555,  0.71649408, -0.42653473, -1.27791003, -0.08681777]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate toy RNA-seq data\n",
    "num_genes = 10\n",
    "num_samples = 5\n",
    "gene_means = np.random.normal(0, 1, size=num_genes)\n",
    "gene_vars = np.random.gamma(1, 1, size=num_genes)\n",
    "data = np.random.normal(gene_means, np.sqrt(gene_vars), size=(num_samples, num_genes))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from typing import Tuple\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scglue.models as GLUE\n",
    "import pandas as pd\n",
    "\n",
    "# Define the RNAseq data\n",
    "n_samples = 100\n",
    "n_genes = 200\n",
    "rna_data = torch.randn(n_genes, n_samples)\n",
    "\n",
    "# Define the data loader\n",
    "data = TensorDataset(rna_data)\n",
    "data_loader = DataLoader(data, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the encoder and decoder\n",
    "# latent_dim = 2\n",
    "# encoder = GLUE.glue.DataEncoder(n_genes)\n",
    "# decoder = GLUE.glue.DataDecoder(n_genes)\n",
    "\n",
    "# # Define the prior\n",
    "# prior = GLUE.glue.Prior()\n",
    "\n",
    "# # Define the optimizer\n",
    "# lr = 1e-3\n",
    "# optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "\n",
    "# # Define the training loop\n",
    "# n_epochs = 100\n",
    "# for epoch in range(n_epochs):\n",
    "#     epoch_loss = 0.0\n",
    "#     for batch_idx, batch in enumerate(data_loader):\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Get the data\n",
    "#         x = batch[0]\n",
    "\n",
    "#         # Forward pass\n",
    "#         q_z_x, z_l = encoder(x, torch.empty(0), lazy_normalizer=False)\n",
    "#         z = q_z_x.rsample()\n",
    "#         p_x_z = decoder(*z.shape, z, torch.zeros_like(z), torch.empty(0))\n",
    "\n",
    "#         # Compute the loss\n",
    "#         kl_divergence = torch.distributions.kl.kl_divergence(q_z_x, prior()).mean()\n",
    "#         reconstruction_loss = -p_x_z.log_prob(x).mean()\n",
    "#         loss = kl_divergence + reconstruction_loss\n",
    "\n",
    "#         # Backward pass and optimization step\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Update the epoch loss\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#     # Print the epoch loss\n",
    "#     print(f\"Epoch {epoch}: Loss = {epoch_loss / (batch_idx+1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "from abc import abstractmethod\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "EPS = 1e-8\n",
    "\n",
    "\n",
    "class DataEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, in_features: int, out_features: int,\n",
    "            h_depth: int = 2, h_dim: int = 256,\n",
    "            dropout: float = 0.2\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.h_depth = h_depth\n",
    "        ptr_dim = in_features\n",
    "        for layer in range(self.h_depth):\n",
    "            setattr(self, f\"linear_{layer}\", torch.nn.Linear(ptr_dim, h_dim))\n",
    "            setattr(self, f\"act_{layer}\", torch.nn.LeakyReLU(negative_slope=0.2))\n",
    "            setattr(self, f\"bn_{layer}\", torch.nn.BatchNorm1d(h_dim))\n",
    "            setattr(self, f\"dropout_{layer}\", torch.nn.Dropout(p=dropout))\n",
    "            ptr_dim = h_dim\n",
    "        self.loc = torch.nn.Linear(ptr_dim, out_features)\n",
    "        self.std_lin = torch.nn.Linear(ptr_dim, out_features)\n",
    "\n",
    "    def compute_l(self, x: torch.Tensor) -> Optional[torch.Tensor]:\n",
    "        return None\n",
    "\n",
    "    def normalize(\n",
    "            self, x: torch.Tensor, l: Optional[torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        return x\n",
    "    \n",
    "    def forward(\n",
    "            self, x: torch.Tensor, xrep: torch.Tensor,\n",
    "            lazy_normalizer: bool = True\n",
    "    ) -> Tuple[D.Normal, Optional[torch.Tensor]]:\n",
    "        if xrep.numel():\n",
    "            l = None if lazy_normalizer else self.compute_l(x)\n",
    "            ptr = xrep\n",
    "        else:\n",
    "            l = self.compute_l(x)\n",
    "            ptr = self.normalize(x, l)\n",
    "        for layer in range(self.h_depth):\n",
    "            ptr = getattr(self, f\"linear_{layer}\")(ptr)\n",
    "            ptr = getattr(self, f\"act_{layer}\")(ptr)\n",
    "            ptr = getattr(self, f\"bn_{layer}\")(ptr)\n",
    "            ptr = getattr(self, f\"dropout_{layer}\")(ptr)\n",
    "        loc = self.loc(ptr)\n",
    "        std = F.softplus(self.std_lin(ptr)) + EPS\n",
    "        return D.Normal(loc, std), l\n",
    "\n",
    "\n",
    "class DataDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, out_features: int, n_batches: int\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.scale_lin = torch.nn.Parameter(torch.zeros(1, n_batches, out_features))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "        self.log_theta = torch.nn.Parameter(torch.zeros(n_batches, out_features))\n",
    "        self.lin_dec = torch.nn.Linear(out_features, out_features)\n",
    "\n",
    "    def forward(\n",
    "            self, u: torch.Tensor, \n",
    "            # v: torch.Tensor,\n",
    "            b: torch.Tensor, \n",
    "            l: torch.Tensor\n",
    "    ) -> D.NegativeBinomial:\n",
    "        # scale = F.softplus(self.scale_lin[b])\n",
    "        logit_mu = self.lin_dec(u)\n",
    "        print(logit_mu.size())\n",
    "        print(u.size())\n",
    "        # logit_mu = scale * (u @ v.t()) + self.bias[b]\n",
    "        mu = F.softmax(logit_mu, dim=1) * l\n",
    "        log_theta = self.log_theta[b]\n",
    "        return D.NegativeBinomial(\n",
    "            log_theta.exp(),\n",
    "            logits=(mu + EPS).log() - log_theta\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class Prior(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, loc: float = 0.0, std: float = 1.0\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        loc = torch.as_tensor(loc, dtype=torch.get_default_dtype())\n",
    "        std = torch.as_tensor(std, dtype=torch.get_default_dtype())\n",
    "        self.register_buffer(\"loc\", loc)\n",
    "        self.register_buffer(\"std\", std)\n",
    "\n",
    "    def forward(self) -> D.Normal:\n",
    "        return D.Normal(self.loc, self.std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sc import *\n",
    "num_batch = 5\n",
    "latent_feature = 10\n",
    "num_epochs = 100\n",
    "encoder = DataEncoder(in_features=n_samples, out_features=latent_feature)\n",
    "decoder = DataDecoder(out_features=latent_feature, n_batches=num_batch)\n",
    "# # Define the optimizer\n",
    "lr = 0.1\n",
    "optimizer = Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "prior = Prior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(\n",
    "    x: torch.Tensor,\n",
    "    encoder: torch.nn.Module,\n",
    "    decoder: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    batch_size: int,\n",
    "    prior: Optional[torch.nn.Module] = None,\n",
    "    beta: float = 1.0,\n",
    "    num_epochs: int = 1000\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # Set the model to training mode\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    # Create a data loader\n",
    "    dataset = x\n",
    "    loader = data_loader\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass through the encoder and compute the latent distribution\n",
    "            x_batch = batch[0]\n",
    "            q_z_x, _ = encoder(x_batch, torch.tensor([]), lazy_normalizer=True)\n",
    "\n",
    "            # Sample a latent variable z from the distribution\n",
    "            z = q_z_x.rsample()\n",
    "\n",
    "            # If a prior is given, compute the KL divergence between q(z|x) and p(z)\n",
    "            if prior is not None:\n",
    "                p_z = prior()\n",
    "                kl_div = D.kl.kl_divergence(q_z_x, p_z).sum(dim=1)\n",
    "                kl_loss = beta * kl_div.mean()\n",
    "            else:\n",
    "                kl_loss = 0.0\n",
    "\n",
    "            # Decode the latent variable to get the reconstructed data distribution\n",
    "            b = torch.zeros(batch_size, dtype=torch.long)\n",
    "\n",
    "            p_x_z = decoder(z, b, l=torch.ones_like(x_batch))\n",
    "\n",
    "            # Compute the negative log likelihood loss between the original data and the reconstructed data\n",
    "            x_log_probs = p_x_z.log_prob(x_batch).sum(dim=1)\n",
    "            nll_loss = -x_log_probs.mean()\n",
    "\n",
    "            # Compute the total loss and backpropagate the gradients\n",
    "            loss = nll_loss + kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {epoch_loss/len(dataset):.6f}\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Encode the data to get the latent variables\n",
    "    with torch.no_grad():\n",
    "        q_z_x, _ = encoder(x, torch.tensor([]), lazy_normalizer=True)\n",
    "        z = q_z_x.rsample()\n",
    "\n",
    "        # Decode the latent variables to get the reconstructed data distribution\n",
    "        b = torch.zeros(x.size(0), dtype=torch.long)\n",
    "        p_x_z = decoder(z, b, l=torch.ones_like(x))\n",
    "\n",
    "        # Compute the negative log likelihood loss between the original data and the reconstructed data\n",
    "        x_log_probs = p_x_z.log_prob(x).sum(dim=1)\n",
    "        nll_loss = -x_log_probs.mean()\n",
    "\n",
    "    return z, p_x_z.mean, nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_vae \u001b[39m=\u001b[39m train_autoencoder(data, encoder, decoder, optimizer, batch_size\u001b[39m=\u001b[39;49mnum_batch, prior\u001b[39m=\u001b[39;49mprior, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[63], line 43\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(x, encoder, decoder, optimizer, batch_size, prior, beta, num_epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m# Decode the latent variable to get the reconstructed data distribution\u001b[39;00m\n\u001b[1;32m     41\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(batch_size, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m---> 43\u001b[0m p_x_z \u001b[39m=\u001b[39m decoder(z, b, l\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(x_batch))\n\u001b[1;32m     45\u001b[0m \u001b[39m# Compute the negative log likelihood loss between the original data and the reconstructed data\u001b[39;00m\n\u001b[1;32m     46\u001b[0m x_log_probs \u001b[39m=\u001b[39m p_x_z\u001b[39m.\u001b[39mlog_prob(x_batch)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[61], line 77\u001b[0m, in \u001b[0;36mDataDecoder.forward\u001b[0;34m(self, u, b, l)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mprint\u001b[39m(u\u001b[39m.\u001b[39msize())\n\u001b[1;32m     76\u001b[0m \u001b[39m# logit_mu = scale * (u @ v.t()) + self.bias[b]\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m mu \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49msoftmax(logit_mu, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m l\n\u001b[1;32m     78\u001b[0m log_theta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_theta[b]\n\u001b[1;32m     79\u001b[0m \u001b[39mreturn\u001b[39;00m D\u001b[39m.\u001b[39mNegativeBinomial(\n\u001b[1;32m     80\u001b[0m     log_theta\u001b[39m.\u001b[39mexp(),\n\u001b[1;32m     81\u001b[0m     logits\u001b[39m=\u001b[39m(mu \u001b[39m+\u001b[39m EPS)\u001b[39m.\u001b[39mlog() \u001b[39m-\u001b[39m log_theta\n\u001b[1;32m     82\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "test_vae = train_autoencoder(data, encoder, decoder, optimizer, batch_size=num_batch, prior=prior, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(\n",
    "        data: torch.Tensor, model: torch.nn.Module,\n",
    "        epochs: int = 100, batch_size: int = 32,\n",
    "        learning_rate: float = 1e-3\n",
    ") -> Tuple[torch.nn.Module, torch.Tensor]:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    n_batches = (data.size(0) + batch_size - 1) // batch_size\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in range(n_batches):\n",
    "            start = batch * batch_size\n",
    "            end = min((batch+1) * batch_size, data.size(0))\n",
    "            x = data[start:end]\n",
    "            optimizer.zero_grad()\n",
    "            dist, _ = model(x, x.new_empty(0))\n",
    "            z = dist.rsample()\n",
    "            recon_dist = model.decoder(z, z.new_empty(0), torch.zeros_like(x), torch.ones_like(x))\n",
    "            loss = -recon_dist.log_prob(x).mean()\n",
    "            kl_loss = D.kl_divergence(dist, model.prior()).mean()\n",
    "            loss += kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * (end - start)\n",
    "        epoch_loss /= data.size(0)\n",
    "        print(f\"Epoch {epoch}/{epochs}: Loss={epoch_loss:.4f}\")\n",
    "    return model, z.detach()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glue_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c204853792b21fcac3f33003e8cf0635bb37268bd89f72590d4a7ab30dd59ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
