{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/mambaforge/envs/glue_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"\"  # Set the GPU\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(device) # change allocation of current GPU\n",
    "# torch.cuda.set_per_process_memory_fraction(0.4, device=device)\n",
    "\n",
    "# print('Device:', device)\n",
    "# print('Current cuda device:', torch.cuda.current_device())\n",
    "# print('Available devices:', torch.cuda.device_count())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GPU is not available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"# DEVICE {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(\"- Memory Usage:\")\n",
    "        print(torch.cuda.list_gpu_processes(i))\n",
    "        print(\"------------------------\")\n",
    "        print(f\"  Allocated: {round(torch.cuda.memory_allocated(i)/1024**3,1)} GB\")\n",
    "        print(f\"  Cached:    {round(torch.cuda.memory_reserved(i)/1024**3,1)} GB\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"# GPU is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import VanillaVAE\n",
    "from models import BaseVAE\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5, 4, ..., 0, 3, 4],\n",
       "       [2, 0, 1, ..., 1, 5, 0],\n",
       "       [2, 3, 5, ..., 3, 1, 3],\n",
       "       ...,\n",
       "       [2, 3, 1, ..., 1, 4, 3],\n",
       "       [5, 4, 1, ..., 1, 0, 3],\n",
       "       [0, 5, 0, ..., 2, 1, 0]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "input_data = np.random.randint(0,6,(100,50))\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.tensor(input_data[81:]).float()\n",
    "train_input = torch.tensor(input_data[0:80]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dim = train_input.size(0)*train_input.size(1)\n",
    "data_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 4\n",
    "hidden_dim = 8\n",
    "batch_size = 10\n",
    "x_dim = len(input_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_loader = DataLoader(dataset=train, batch_size = batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(dataset=test, batch_size = batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 5., 3., 2., 4., 1., 1., 0., 4., 1., 0., 4., 2., 0., 3., 3., 4., 1.,\n",
      "         2., 5., 3., 2., 4., 3., 0., 5., 1., 5., 0., 5., 3., 0., 0., 1., 1., 3.,\n",
      "         0., 2., 3., 2., 1., 4., 3., 2., 4., 2., 2., 4., 5., 2.],\n",
      "        [0., 0., 5., 3., 2., 1., 4., 0., 4., 1., 2., 1., 3., 3., 1., 0., 1., 3.,\n",
      "         4., 0., 1., 0., 3., 3., 3., 5., 1., 0., 0., 2., 0., 3., 4., 3., 3., 5.,\n",
      "         2., 0., 5., 2., 5., 5., 4., 3., 1., 4., 4., 3., 3., 5.],\n",
      "        [5., 1., 1., 3., 3., 1., 4., 0., 0., 3., 0., 2., 0., 1., 0., 4., 2., 2.,\n",
      "         3., 0., 3., 3., 3., 1., 3., 1., 4., 0., 2., 1., 0., 0., 0., 1., 4., 3.,\n",
      "         5., 0., 2., 3., 2., 1., 4., 1., 1., 5., 3., 0., 3., 0.],\n",
      "        [0., 3., 4., 0., 5., 1., 0., 1., 4., 4., 2., 4., 1., 3., 5., 1., 5., 4.,\n",
      "         5., 0., 0., 2., 5., 5., 1., 1., 4., 5., 1., 1., 5., 3., 2., 5., 1., 4.,\n",
      "         5., 2., 0., 0., 2., 4., 2., 1., 3., 0., 1., 4., 4., 3.],\n",
      "        [4., 2., 5., 1., 1., 1., 3., 5., 5., 1., 5., 2., 2., 4., 0., 1., 2., 2.,\n",
      "         1., 2., 0., 3., 0., 4., 2., 4., 5., 5., 1., 0., 4., 4., 3., 3., 4., 4.,\n",
      "         5., 2., 3., 1., 3., 2., 1., 2., 0., 4., 2., 5., 2., 2.],\n",
      "        [2., 3., 0., 3., 2., 5., 3., 5., 0., 1., 4., 5., 3., 1., 4., 2., 5., 4.,\n",
      "         5., 5., 5., 1., 0., 4., 2., 4., 3., 0., 3., 4., 4., 1., 4., 5., 5., 3.,\n",
      "         0., 1., 1., 3., 3., 2., 4., 0., 3., 2., 4., 5., 1., 4.],\n",
      "        [2., 4., 3., 5., 0., 0., 4., 2., 4., 3., 1., 4., 2., 3., 5., 5., 3., 2.,\n",
      "         2., 0., 3., 0., 1., 3., 1., 2., 2., 2., 0., 5., 0., 0., 2., 4., 0., 0.,\n",
      "         5., 2., 1., 2., 3., 3., 4., 3., 4., 0., 3., 3., 5., 3.],\n",
      "        [5., 1., 1., 5., 1., 4., 3., 0., 4., 1., 1., 0., 2., 2., 0., 3., 0., 2.,\n",
      "         4., 5., 2., 4., 5., 1., 2., 3., 0., 4., 4., 2., 3., 1., 1., 2., 0., 4.,\n",
      "         5., 4., 0., 1., 0., 2., 2., 0., 0., 3., 1., 0., 1., 4.],\n",
      "        [4., 4., 3., 2., 3., 3., 1., 2., 5., 0., 2., 5., 2., 4., 5., 3., 0., 3.,\n",
      "         3., 0., 2., 1., 5., 1., 0., 2., 3., 1., 4., 2., 0., 2., 0., 0., 0., 5.,\n",
      "         3., 0., 0., 0., 2., 0., 4., 2., 1., 3., 2., 0., 4., 3.],\n",
      "        [2., 3., 1., 2., 4., 1., 0., 5., 1., 2., 2., 3., 2., 3., 3., 0., 5., 2.,\n",
      "         3., 1., 4., 4., 1., 4., 3., 5., 1., 1., 0., 1., 4., 5., 1., 2., 5., 2.,\n",
      "         5., 2., 3., 4., 2., 5., 0., 3., 2., 0., 0., 5., 2., 1.]])\n",
      "0\n",
      "tensor([[0., 3., 0., 2., 4., 4., 1., 1., 3., 5., 2., 1., 5., 5., 3., 2., 4., 1.,\n",
      "         3., 5., 3., 4., 4., 2., 3., 5., 2., 2., 0., 1., 2., 4., 5., 0., 2., 0.,\n",
      "         3., 4., 4., 4., 1., 4., 0., 3., 4., 0., 3., 2., 4., 2.],\n",
      "        [1., 1., 2., 3., 5., 0., 4., 3., 4., 5., 1., 3., 3., 3., 2., 3., 3., 5.,\n",
      "         1., 1., 1., 0., 1., 5., 3., 4., 4., 0., 4., 5., 2., 4., 3., 1., 2., 3.,\n",
      "         1., 1., 1., 2., 3., 2., 5., 0., 2., 1., 4., 4., 2., 3.],\n",
      "        [3., 2., 3., 2., 3., 3., 3., 0., 1., 1., 2., 1., 4., 4., 1., 3., 1., 3.,\n",
      "         1., 5., 3., 5., 0., 0., 2., 3., 4., 0., 1., 3., 3., 2., 4., 3., 3., 1.,\n",
      "         3., 5., 0., 2., 4., 4., 4., 1., 2., 5., 1., 5., 4., 1.],\n",
      "        [4., 4., 1., 4., 4., 4., 4., 2., 0., 2., 1., 5., 4., 0., 2., 5., 0., 4.,\n",
      "         1., 4., 3., 0., 2., 1., 0., 5., 0., 4., 4., 3., 1., 4., 1., 3., 3., 0.,\n",
      "         2., 5., 5., 0., 4., 1., 5., 3., 0., 3., 4., 3., 0., 4.],\n",
      "        [1., 2., 0., 4., 0., 2., 3., 2., 0., 1., 2., 0., 3., 2., 1., 4., 3., 5.,\n",
      "         5., 4., 5., 3., 4., 2., 2., 3., 1., 4., 4., 0., 0., 4., 4., 1., 2., 2.,\n",
      "         3., 0., 3., 5., 5., 0., 5., 0., 2., 1., 2., 4., 0., 3.],\n",
      "        [2., 2., 4., 2., 0., 2., 0., 5., 3., 4., 3., 3., 3., 5., 3., 0., 3., 3.,\n",
      "         3., 4., 5., 5., 0., 0., 5., 4., 0., 5., 5., 2., 5., 4., 0., 3., 5., 3.,\n",
      "         5., 1., 1., 1., 3., 0., 5., 3., 4., 4., 4., 1., 0., 4.],\n",
      "        [2., 5., 5., 0., 4., 2., 5., 0., 3., 4., 1., 5., 5., 2., 3., 4., 5., 4.,\n",
      "         5., 0., 0., 2., 4., 3., 5., 1., 3., 0., 3., 0., 3., 1., 2., 3., 1., 1.,\n",
      "         5., 2., 4., 0., 3., 5., 0., 2., 0., 1., 4., 5., 3., 4.],\n",
      "        [1., 4., 1., 4., 1., 3., 4., 0., 5., 2., 5., 1., 5., 3., 4., 5., 0., 2.,\n",
      "         3., 5., 2., 4., 2., 5., 3., 2., 3., 1., 3., 5., 5., 1., 4., 0., 1., 0.,\n",
      "         3., 5., 5., 4., 5., 3., 5., 3., 2., 3., 1., 1., 1., 4.],\n",
      "        [1., 2., 5., 3., 3., 5., 4., 4., 4., 2., 1., 1., 1., 4., 4., 1., 1., 3.,\n",
      "         5., 3., 4., 2., 3., 2., 0., 4., 2., 2., 0., 5., 1., 5., 4., 2., 4., 3.,\n",
      "         5., 1., 0., 1., 1., 0., 4., 4., 0., 1., 2., 2., 0., 4.],\n",
      "        [0., 1., 2., 0., 0., 3., 3., 5., 1., 0., 1., 4., 2., 2., 1., 4., 1., 4.,\n",
      "         2., 0., 5., 0., 4., 4., 4., 1., 3., 0., 1., 4., 2., 5., 5., 0., 5., 2.,\n",
      "         0., 1., 2., 0., 4., 1., 5., 4., 4., 4., 2., 2., 0., 3.]])\n",
      "1\n",
      "tensor([[1., 3., 2., 1., 4., 3., 5., 3., 4., 0., 4., 4., 2., 4., 3., 1., 1., 3.,\n",
      "         1., 0., 3., 5., 2., 0., 2., 3., 1., 1., 3., 5., 2., 5., 1., 3., 1., 3.,\n",
      "         4., 5., 4., 2., 3., 5., 1., 1., 5., 4., 0., 1., 4., 1.],\n",
      "        [0., 5., 1., 1., 0., 0., 0., 0., 5., 3., 2., 3., 3., 3., 0., 1., 3., 4.,\n",
      "         0., 0., 1., 0., 0., 0., 5., 2., 4., 3., 4., 1., 4., 2., 3., 2., 3., 0.,\n",
      "         0., 0., 1., 0., 3., 1., 1., 2., 2., 2., 1., 2., 0., 5.],\n",
      "        [1., 0., 3., 1., 5., 4., 4., 5., 5., 5., 5., 5., 2., 3., 0., 5., 0., 5.,\n",
      "         1., 0., 2., 1., 0., 0., 2., 4., 5., 0., 0., 3., 5., 5., 1., 3., 4., 0.,\n",
      "         0., 2., 5., 3., 1., 5., 4., 3., 0., 5., 3., 4., 4., 3.],\n",
      "        [3., 5., 2., 3., 4., 5., 0., 1., 3., 3., 2., 5., 5., 1., 1., 3., 1., 1.,\n",
      "         3., 1., 5., 2., 3., 5., 4., 4., 3., 4., 4., 2., 5., 0., 2., 2., 4., 3.,\n",
      "         4., 2., 3., 3., 3., 5., 3., 3., 2., 0., 5., 2., 0., 1.],\n",
      "        [3., 5., 1., 5., 1., 3., 5., 3., 2., 4., 0., 5., 3., 1., 4., 2., 3., 3.,\n",
      "         2., 3., 5., 0., 3., 1., 3., 2., 4., 1., 5., 5., 0., 0., 2., 0., 4., 4.,\n",
      "         1., 3., 2., 5., 1., 2., 5., 2., 3., 3., 4., 3., 4., 5.],\n",
      "        [3., 3., 4., 5., 0., 3., 0., 2., 4., 3., 3., 5., 2., 3., 4., 0., 0., 5.,\n",
      "         3., 1., 4., 4., 0., 0., 0., 4., 5., 0., 3., 0., 5., 1., 1., 3., 1., 1.,\n",
      "         5., 4., 1., 5., 5., 1., 1., 2., 3., 5., 5., 4., 1., 0.],\n",
      "        [3., 0., 4., 1., 3., 3., 4., 2., 1., 3., 2., 2., 3., 1., 1., 4., 2., 4.,\n",
      "         0., 5., 4., 3., 3., 5., 4., 1., 5., 3., 3., 1., 2., 0., 5., 2., 5., 2.,\n",
      "         1., 3., 4., 0., 4., 2., 3., 3., 3., 5., 5., 0., 1., 1.],\n",
      "        [1., 2., 0., 4., 0., 1., 3., 2., 1., 3., 4., 4., 3., 0., 2., 1., 2., 1.,\n",
      "         3., 2., 4., 1., 4., 5., 4., 2., 5., 0., 3., 3., 4., 1., 3., 2., 5., 2.,\n",
      "         0., 5., 2., 2., 0., 3., 1., 5., 4., 3., 2., 0., 2., 0.],\n",
      "        [5., 3., 2., 0., 2., 3., 5., 5., 1., 5., 4., 4., 4., 0., 1., 0., 2., 4.,\n",
      "         0., 5., 1., 5., 2., 1., 4., 1., 3., 2., 1., 0., 4., 5., 0., 4., 2., 3.,\n",
      "         1., 4., 0., 1., 1., 3., 0., 5., 5., 1., 0., 5., 3., 0.],\n",
      "        [0., 0., 0., 2., 4., 3., 5., 2., 2., 5., 0., 5., 1., 3., 5., 4., 3., 2.,\n",
      "         4., 3., 1., 0., 1., 2., 1., 0., 3., 5., 0., 4., 1., 5., 1., 2., 2., 1.,\n",
      "         5., 4., 5., 1., 3., 4., 5., 1., 2., 0., 2., 2., 4., 4.]])\n",
      "2\n",
      "tensor([[1., 3., 4., 0., 4., 0., 4., 1., 0., 3., 2., 0., 2., 4., 5., 3., 5., 5.,\n",
      "         2., 0., 3., 4., 3., 3., 4., 0., 4., 3., 3., 4., 5., 0., 4., 1., 3., 1.,\n",
      "         4., 1., 4., 5., 2., 0., 2., 0., 0., 3., 3., 5., 1., 2.],\n",
      "        [3., 3., 1., 5., 5., 2., 5., 0., 2., 5., 3., 3., 2., 5., 5., 3., 2., 4.,\n",
      "         0., 5., 0., 2., 2., 4., 0., 4., 2., 4., 0., 4., 2., 4., 2., 2., 5., 2.,\n",
      "         1., 0., 2., 0., 3., 2., 1., 3., 3., 1., 2., 5., 5., 0.],\n",
      "        [1., 3., 0., 1., 3., 0., 5., 1., 5., 0., 1., 0., 1., 3., 5., 1., 4., 4.,\n",
      "         2., 5., 4., 5., 0., 3., 5., 2., 1., 5., 3., 3., 5., 2., 1., 0., 2., 0.,\n",
      "         4., 0., 4., 4., 0., 4., 0., 1., 4., 3., 5., 3., 2., 2.],\n",
      "        [5., 5., 4., 5., 2., 0., 2., 3., 4., 0., 3., 4., 0., 2., 5., 0., 4., 4.,\n",
      "         4., 5., 5., 0., 0., 5., 5., 1., 2., 3., 2., 3., 1., 1., 3., 3., 2., 5.,\n",
      "         4., 4., 3., 2., 0., 3., 1., 2., 0., 4., 2., 2., 4., 0.],\n",
      "        [5., 3., 4., 3., 2., 5., 3., 0., 3., 1., 0., 3., 1., 0., 1., 2., 3., 2.,\n",
      "         1., 0., 2., 5., 5., 4., 4., 5., 3., 5., 5., 2., 5., 5., 3., 4., 5., 5.,\n",
      "         0., 5., 0., 5., 5., 0., 4., 2., 1., 1., 2., 2., 0., 1.],\n",
      "        [1., 0., 1., 1., 4., 2., 2., 2., 4., 4., 3., 0., 2., 4., 5., 5., 1., 3.,\n",
      "         0., 2., 5., 4., 4., 5., 2., 5., 3., 4., 1., 5., 2., 0., 0., 2., 0., 4.,\n",
      "         5., 1., 2., 0., 0., 5., 2., 2., 2., 2., 3., 4., 2., 5.],\n",
      "        [5., 0., 1., 2., 2., 5., 2., 4., 2., 4., 0., 4., 3., 1., 5., 4., 1., 3.,\n",
      "         4., 4., 4., 2., 4., 5., 2., 5., 3., 4., 3., 4., 4., 3., 1., 0., 1., 5.,\n",
      "         1., 5., 0., 2., 3., 0., 1., 2., 1., 1., 3., 4., 1., 0.],\n",
      "        [5., 0., 1., 2., 1., 3., 4., 2., 1., 0., 1., 2., 1., 3., 0., 0., 1., 4.,\n",
      "         3., 5., 4., 5., 4., 5., 4., 3., 5., 4., 4., 1., 1., 2., 1., 1., 2., 4.,\n",
      "         4., 2., 5., 2., 5., 0., 1., 1., 1., 2., 0., 0., 5., 1.],\n",
      "        [2., 0., 0., 4., 4., 2., 3., 3., 5., 2., 0., 5., 1., 4., 2., 3., 2., 3.,\n",
      "         2., 4., 3., 3., 0., 2., 2., 2., 3., 0., 1., 2., 1., 1., 1., 3., 0., 2.,\n",
      "         0., 5., 1., 2., 2., 4., 3., 5., 5., 4., 2., 5., 5., 5.],\n",
      "        [5., 4., 4., 5., 4., 2., 4., 2., 1., 4., 2., 4., 5., 5., 0., 4., 2., 5.,\n",
      "         4., 5., 5., 2., 4., 5., 4., 1., 3., 2., 2., 0., 3., 4., 2., 0., 5., 3.,\n",
      "         4., 1., 3., 1., 3., 2., 0., 1., 0., 0., 3., 1., 0., 2.]])\n",
      "3\n",
      "tensor([[5., 3., 5., 1., 3., 0., 0., 5., 1., 0., 5., 3., 3., 2., 0., 5., 0., 4.,\n",
      "         5., 3., 1., 1., 1., 5., 5., 1., 1., 4., 4., 3., 1., 0., 3., 1., 4., 4.,\n",
      "         4., 0., 4., 2., 1., 2., 1., 2., 1., 3., 2., 5., 3., 4.],\n",
      "        [3., 4., 3., 0., 4., 2., 2., 2., 4., 2., 2., 3., 1., 0., 0., 1., 4., 0.,\n",
      "         1., 3., 4., 0., 1., 5., 3., 1., 0., 5., 4., 2., 5., 4., 4., 1., 0., 4.,\n",
      "         5., 2., 2., 0., 3., 5., 5., 1., 4., 0., 0., 2., 4., 1.],\n",
      "        [5., 5., 1., 5., 0., 5., 1., 1., 5., 3., 1., 5., 1., 2., 4., 2., 0., 1.,\n",
      "         2., 0., 2., 0., 0., 4., 2., 0., 4., 5., 1., 4., 2., 1., 5., 1., 4., 3.,\n",
      "         3., 1., 0., 0., 2., 2., 3., 2., 5., 5., 1., 5., 4., 5.],\n",
      "        [5., 5., 4., 0., 2., 2., 0., 0., 2., 5., 0., 5., 2., 2., 0., 4., 1., 3.,\n",
      "         5., 2., 3., 3., 5., 0., 4., 1., 2., 0., 4., 4., 5., 5., 4., 2., 1., 4.,\n",
      "         0., 3., 4., 2., 4., 1., 0., 0., 3., 2., 1., 1., 1., 3.],\n",
      "        [3., 4., 1., 2., 2., 5., 3., 3., 1., 2., 1., 0., 4., 1., 5., 5., 0., 1.,\n",
      "         0., 5., 4., 4., 2., 5., 5., 5., 4., 2., 4., 3., 1., 2., 3., 0., 1., 3.,\n",
      "         5., 5., 0., 1., 2., 1., 2., 1., 3., 1., 0., 4., 1., 3.],\n",
      "        [3., 5., 0., 5., 4., 5., 2., 2., 2., 0., 2., 5., 5., 3., 4., 1., 0., 1.,\n",
      "         0., 5., 5., 4., 0., 2., 2., 2., 5., 4., 5., 3., 0., 2., 1., 1., 1., 2.,\n",
      "         5., 2., 0., 0., 4., 3., 5., 5., 2., 0., 4., 1., 0., 0.],\n",
      "        [4., 4., 4., 1., 3., 1., 5., 3., 5., 5., 0., 5., 2., 5., 0., 2., 3., 4.,\n",
      "         3., 2., 4., 4., 4., 2., 3., 2., 0., 3., 3., 2., 2., 1., 2., 0., 1., 0.,\n",
      "         0., 1., 3., 0., 1., 1., 2., 0., 4., 4., 5., 1., 0., 5.],\n",
      "        [3., 3., 2., 0., 2., 1., 3., 2., 2., 4., 0., 4., 0., 4., 1., 0., 2., 4.,\n",
      "         4., 4., 3., 2., 1., 3., 3., 4., 0., 4., 4., 4., 2., 2., 5., 4., 3., 2.,\n",
      "         0., 1., 1., 1., 3., 2., 1., 5., 3., 4., 1., 0., 4., 5.],\n",
      "        [3., 5., 3., 0., 2., 0., 3., 3., 5., 0., 4., 2., 3., 3., 4., 4., 1., 3.,\n",
      "         1., 0., 4., 5., 4., 2., 2., 0., 0., 1., 5., 0., 2., 0., 4., 4., 3., 5.,\n",
      "         2., 3., 3., 5., 3., 4., 4., 4., 3., 3., 3., 4., 2., 1.],\n",
      "        [2., 2., 1., 5., 0., 5., 0., 3., 1., 3., 0., 2., 0., 1., 2., 2., 2., 0.,\n",
      "         4., 0., 4., 3., 4., 5., 3., 5., 1., 4., 2., 3., 2., 2., 5., 0., 4., 1.,\n",
      "         2., 5., 1., 4., 2., 2., 3., 1., 5., 2., 1., 2., 3., 2.]])\n",
      "4\n",
      "tensor([[5., 3., 0., 5., 0., 1., 3., 1., 4., 0., 0., 1., 1., 5., 0., 3., 5., 3.,\n",
      "         5., 5., 1., 0., 2., 1., 2., 4., 2., 2., 2., 2., 2., 3., 4., 5., 3., 5.,\n",
      "         2., 1., 3., 3., 2., 4., 0., 1., 5., 3., 4., 2., 1., 1.],\n",
      "        [3., 3., 2., 4., 4., 3., 1., 2., 3., 5., 1., 3., 2., 5., 5., 2., 2., 3.,\n",
      "         5., 5., 4., 3., 2., 0., 3., 0., 0., 5., 4., 3., 4., 2., 0., 0., 5., 1.,\n",
      "         2., 0., 1., 4., 5., 2., 0., 4., 3., 5., 3., 5., 3., 3.],\n",
      "        [5., 4., 1., 1., 3., 5., 3., 3., 0., 4., 4., 1., 2., 4., 0., 2., 4., 1.,\n",
      "         1., 0., 5., 3., 0., 1., 1., 1., 5., 0., 3., 5., 0., 1., 4., 4., 5., 1.,\n",
      "         4., 1., 1., 2., 1., 3., 2., 0., 4., 1., 2., 5., 1., 0.],\n",
      "        [0., 0., 2., 0., 4., 3., 4., 0., 5., 3., 4., 0., 1., 1., 5., 0., 5., 5.,\n",
      "         2., 0., 1., 2., 0., 3., 5., 1., 3., 5., 0., 0., 4., 1., 5., 2., 1., 5.,\n",
      "         0., 1., 3., 3., 3., 5., 5., 1., 3., 1., 3., 3., 2., 2.],\n",
      "        [3., 1., 4., 2., 5., 5., 3., 2., 0., 4., 2., 3., 2., 5., 0., 0., 4., 2.,\n",
      "         3., 2., 0., 1., 1., 3., 4., 3., 1., 4., 0., 1., 1., 2., 1., 5., 2., 4.,\n",
      "         2., 4., 3., 4., 2., 2., 0., 4., 2., 4., 0., 2., 1., 0.],\n",
      "        [4., 2., 4., 4., 4., 1., 5., 4., 2., 2., 0., 4., 1., 5., 2., 0., 5., 4.,\n",
      "         3., 5., 1., 4., 3., 1., 3., 4., 0., 4., 3., 3., 1., 0., 3., 2., 4., 4.,\n",
      "         2., 1., 5., 3., 1., 2., 4., 2., 2., 5., 0., 2., 1., 0.],\n",
      "        [2., 0., 2., 0., 3., 0., 2., 0., 2., 1., 1., 4., 3., 0., 1., 4., 1., 3.,\n",
      "         0., 2., 0., 1., 2., 3., 1., 1., 4., 0., 0., 5., 0., 2., 0., 1., 3., 2.,\n",
      "         5., 4., 0., 1., 2., 5., 1., 0., 4., 4., 1., 4., 2., 1.],\n",
      "        [5., 4., 3., 3., 2., 2., 0., 5., 2., 1., 4., 3., 2., 2., 4., 0., 1., 0.,\n",
      "         2., 1., 0., 4., 1., 3., 3., 3., 4., 0., 5., 2., 5., 1., 4., 4., 1., 0.,\n",
      "         3., 5., 1., 2., 1., 1., 4., 4., 2., 5., 4., 3., 0., 3.],\n",
      "        [2., 4., 1., 0., 0., 4., 3., 0., 4., 4., 2., 1., 2., 0., 0., 0., 3., 5.,\n",
      "         3., 3., 2., 1., 1., 1., 1., 0., 1., 2., 5., 4., 0., 4., 4., 2., 1., 0.,\n",
      "         5., 2., 3., 5., 0., 5., 3., 2., 4., 1., 2., 0., 0., 0.],\n",
      "        [2., 0., 1., 0., 3., 5., 5., 1., 5., 1., 3., 2., 5., 5., 5., 0., 4., 1.,\n",
      "         5., 3., 3., 3., 5., 0., 4., 5., 2., 1., 2., 5., 0., 2., 0., 5., 0., 5.,\n",
      "         3., 2., 1., 0., 1., 1., 5., 4., 3., 5., 0., 4., 2., 4.]])\n",
      "5\n",
      "tensor([[0., 3., 4., 3., 1., 2., 2., 3., 1., 5., 4., 3., 3., 4., 5., 3., 1., 4.,\n",
      "         3., 0., 4., 5., 3., 1., 2., 5., 1., 3., 1., 4., 5., 1., 4., 4., 2., 1.,\n",
      "         5., 1., 4., 2., 1., 3., 3., 2., 3., 0., 0., 2., 5., 4.],\n",
      "        [4., 0., 5., 1., 3., 3., 3., 4., 4., 1., 1., 4., 2., 1., 0., 0., 2., 5.,\n",
      "         5., 0., 1., 4., 1., 1., 4., 4., 0., 0., 2., 3., 4., 0., 5., 4., 0., 3.,\n",
      "         4., 0., 1., 4., 4., 3., 4., 2., 1., 4., 4., 1., 2., 4.],\n",
      "        [4., 4., 1., 5., 2., 3., 3., 3., 5., 1., 5., 5., 5., 5., 2., 3., 0., 0.,\n",
      "         1., 1., 2., 5., 0., 5., 5., 4., 5., 1., 2., 3., 4., 4., 1., 2., 0., 0.,\n",
      "         3., 3., 2., 3., 0., 1., 4., 5., 3., 3., 0., 0., 5., 5.],\n",
      "        [0., 1., 4., 1., 2., 0., 2., 4., 3., 2., 4., 2., 3., 3., 3., 0., 0., 1.,\n",
      "         3., 4., 2., 0., 3., 1., 1., 4., 5., 4., 2., 0., 0., 5., 0., 5., 5., 2.,\n",
      "         3., 0., 1., 4., 4., 5., 2., 3., 5., 5., 4., 1., 1., 3.],\n",
      "        [3., 3., 2., 3., 4., 4., 2., 3., 0., 0., 2., 1., 1., 4., 4., 4., 1., 5.,\n",
      "         0., 3., 0., 0., 1., 2., 5., 2., 3., 5., 3., 0., 0., 2., 0., 3., 1., 1.,\n",
      "         0., 0., 5., 2., 5., 4., 2., 1., 1., 1., 2., 3., 5., 4.],\n",
      "        [3., 3., 0., 3., 1., 1., 1., 2., 4., 5., 1., 0., 5., 0., 0., 5., 3., 4.,\n",
      "         0., 4., 2., 1., 4., 3., 1., 3., 2., 5., 4., 4., 0., 1., 4., 1., 3., 5.,\n",
      "         4., 3., 5., 2., 2., 5., 2., 0., 1., 0., 3., 4., 1., 0.],\n",
      "        [3., 1., 3., 3., 0., 3., 5., 2., 4., 0., 0., 1., 4., 5., 1., 1., 2., 2.,\n",
      "         4., 5., 3., 1., 5., 3., 1., 4., 2., 5., 2., 3., 1., 1., 4., 2., 5., 4.,\n",
      "         4., 3., 1., 3., 2., 0., 2., 3., 3., 5., 1., 1., 2., 5.],\n",
      "        [1., 5., 4., 0., 1., 3., 4., 1., 5., 0., 5., 1., 2., 0., 1., 0., 2., 0.,\n",
      "         4., 3., 0., 4., 3., 0., 3., 2., 1., 0., 4., 1., 3., 5., 5., 5., 3., 1.,\n",
      "         5., 4., 1., 4., 1., 1., 4., 3., 2., 5., 5., 0., 3., 4.],\n",
      "        [3., 2., 1., 1., 0., 5., 0., 4., 1., 1., 3., 0., 1., 5., 5., 5., 4., 5.,\n",
      "         3., 3., 4., 0., 1., 1., 1., 4., 0., 3., 5., 4., 0., 2., 0., 3., 1., 5.,\n",
      "         1., 2., 4., 3., 5., 1., 1., 0., 2., 0., 3., 1., 2., 4.],\n",
      "        [2., 2., 2., 0., 2., 2., 0., 2., 5., 5., 1., 4., 1., 0., 4., 1., 5., 4.,\n",
      "         5., 4., 2., 0., 5., 5., 5., 0., 2., 1., 1., 4., 2., 3., 1., 4., 3., 0.,\n",
      "         5., 4., 1., 3., 1., 2., 2., 2., 5., 4., 0., 0., 3., 0.]])\n",
      "6\n",
      "tensor([[2., 5., 1., 0., 4., 3., 4., 2., 0., 0., 5., 5., 5., 2., 2., 2., 0., 3.,\n",
      "         1., 0., 3., 2., 2., 1., 2., 4., 1., 0., 4., 4., 5., 3., 3., 0., 2., 4.,\n",
      "         0., 1., 0., 1., 5., 2., 0., 2., 1., 3., 1., 1., 4., 1.],\n",
      "        [5., 2., 4., 4., 2., 5., 2., 1., 2., 2., 0., 3., 2., 1., 5., 3., 1., 3.,\n",
      "         4., 0., 3., 3., 2., 2., 4., 5., 2., 3., 2., 4., 5., 5., 2., 1., 4., 1.,\n",
      "         5., 1., 3., 2., 5., 1., 5., 3., 4., 3., 1., 0., 3., 5.],\n",
      "        [3., 1., 1., 0., 3., 4., 4., 3., 1., 0., 3., 1., 0., 5., 2., 2., 3., 4.,\n",
      "         3., 2., 2., 3., 2., 5., 1., 1., 3., 1., 5., 4., 2., 2., 0., 1., 2., 3.,\n",
      "         4., 1., 1., 3., 3., 4., 1., 2., 5., 3., 3., 3., 4., 0.],\n",
      "        [4., 2., 0., 2., 1., 4., 1., 5., 3., 1., 2., 4., 2., 5., 2., 2., 3., 5.,\n",
      "         5., 5., 4., 2., 2., 4., 4., 5., 0., 2., 4., 5., 3., 5., 0., 0., 1., 5.,\n",
      "         1., 0., 3., 1., 0., 0., 3., 1., 3., 2., 4., 3., 2., 1.],\n",
      "        [2., 0., 1., 2., 0., 0., 3., 1., 3., 4., 1., 4., 2., 0., 0., 4., 5., 4.,\n",
      "         0., 0., 2., 4., 2., 0., 5., 0., 5., 5., 2., 3., 0., 4., 4., 0., 1., 1.,\n",
      "         4., 0., 2., 5., 1., 3., 1., 2., 5., 0., 1., 1., 5., 0.],\n",
      "        [4., 1., 1., 0., 3., 3., 2., 4., 4., 3., 5., 1., 1., 1., 5., 1., 5., 5.,\n",
      "         1., 1., 1., 1., 1., 0., 2., 2., 3., 2., 0., 5., 4., 1., 3., 1., 1., 1.,\n",
      "         5., 2., 5., 3., 1., 0., 2., 5., 3., 0., 1., 5., 3., 1.],\n",
      "        [0., 0., 5., 3., 4., 1., 4., 0., 0., 1., 2., 3., 2., 1., 3., 5., 2., 4.,\n",
      "         3., 1., 0., 3., 5., 2., 1., 2., 0., 2., 2., 2., 4., 2., 0., 5., 0., 3.,\n",
      "         0., 5., 1., 3., 3., 2., 1., 1., 0., 0., 1., 0., 1., 3.],\n",
      "        [5., 4., 0., 0., 1., 2., 1., 2., 1., 1., 1., 4., 2., 1., 2., 0., 1., 0.,\n",
      "         5., 0., 3., 0., 3., 0., 2., 5., 5., 4., 0., 3., 4., 2., 2., 5., 4., 1.,\n",
      "         0., 4., 5., 3., 1., 4., 0., 2., 5., 4., 4., 2., 1., 5.],\n",
      "        [2., 3., 4., 2., 2., 0., 5., 5., 5., 4., 2., 0., 3., 4., 2., 1., 4., 2.,\n",
      "         1., 2., 2., 0., 0., 4., 5., 5., 2., 3., 3., 3., 1., 2., 1., 3., 3., 5.,\n",
      "         5., 0., 1., 1., 2., 5., 2., 2., 4., 3., 5., 2., 2., 2.],\n",
      "        [2., 3., 5., 0., 4., 2., 0., 3., 3., 1., 2., 5., 1., 0., 2., 1., 0., 1.,\n",
      "         1., 0., 1., 2., 4., 2., 1., 1., 4., 0., 0., 5., 1., 0., 4., 4., 0., 2.,\n",
      "         3., 3., 3., 2., 5., 2., 3., 0., 5., 5., 5., 3., 1., 3.]])\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, x in enumerate(train_loader):\n",
    "    print(x)\n",
    "    print(batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
    "        # self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.FC_var   = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_       = self.LeakyReLU(self.FC_input(x))\n",
    "        # h_       = self.LeakyReLU(self.FC_input2(h_))\n",
    "        mean     = self.FC_mean(h_)\n",
    "        log_var  = self.FC_var(h_)                     # encoder produces mean and log of variance \n",
    "                                                       #             (i.e., parateters of simple tractable normal distribution \"q\"\n",
    "        \n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        # self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h     = self.LeakyReLU(self.FC_hidden(x))\n",
    "        # h     = self.LeakyReLU(self.FC_hidden2(h))\n",
    "        \n",
    "        x_hat = torch.sigmoid(self.FC_output(h))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "        \n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "        \n",
    "                \n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) # takes exponential function (log var -> var)\n",
    "        x_hat = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6242, 0.4938, 0.5504,  ..., 0.6364, 0.7783, 0.5218],\n",
       "         [0.5014, 0.6090, 0.4313,  ..., 0.4894, 0.4942, 0.4171],\n",
       "         [0.5418, 0.5462, 0.4615,  ..., 0.4897, 0.3771, 0.3884],\n",
       "         ...,\n",
       "         [0.6277, 0.4487, 0.5076,  ..., 0.6175, 0.6082, 0.4912],\n",
       "         [0.4431, 0.5234, 0.4125,  ..., 0.5887, 0.2842, 0.2681],\n",
       "         [0.6313, 0.6774, 0.4565,  ..., 0.3652, 0.7372, 0.5550]],\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " tensor([[ 0.9455, -1.6483, -1.1045, -0.0197],\n",
       "         [ 0.6648, -1.0683, -1.2934,  0.1030],\n",
       "         [ 0.5216, -1.4020, -0.8126, -0.1195],\n",
       "         [ 0.3730, -1.0509, -0.2236, -0.2629],\n",
       "         [ 0.8461, -1.6346, -0.5925,  0.0124],\n",
       "         [ 0.6205, -1.3636, -0.6137, -0.1860],\n",
       "         [ 0.7227, -1.2924, -2.0313,  0.1454],\n",
       "         [ 0.3135, -0.5820,  0.0812, -0.3032],\n",
       "         [ 0.6396, -1.8365, -1.3478,  0.0521],\n",
       "         [ 0.7029, -1.4180, -0.7664, -0.2673],\n",
       "         [ 0.8016, -1.5009, -0.2992, -0.3197],\n",
       "         [ 0.8535, -1.6618, -0.8213, -0.0039],\n",
       "         [ 0.7840, -1.7205, -1.1736, -0.0354],\n",
       "         [ 0.4361, -1.3107, -0.3752, -0.1575],\n",
       "         [ 0.5799, -1.2422, -1.4407, -0.3400],\n",
       "         [ 0.0732, -0.8059, -0.8864, -0.0509],\n",
       "         [ 0.5074, -1.2195, -1.5828, -0.1769],\n",
       "         [ 0.4740, -0.9491, -0.5250, -0.2193],\n",
       "         [ 0.6439, -0.9998,  0.1081, -0.2031],\n",
       "         [ 0.3988, -0.8415, -0.1387, -0.2437],\n",
       "         [ 0.9281, -1.5308, -0.3477, -0.0294],\n",
       "         [ 0.0223, -0.8470, -0.5963, -0.1852],\n",
       "         [-0.0777, -0.8182, -1.3100,  0.2880],\n",
       "         [ 0.8641, -1.9258, -1.3775,  0.0181],\n",
       "         [ 0.2247, -0.8433, -1.3850, -0.0523],\n",
       "         [ 0.3585, -0.8634, -0.7487,  0.0366],\n",
       "         [ 0.6769, -1.4294, -1.3598, -0.1031],\n",
       "         [ 0.6026, -1.1050, -1.3076, -0.0796],\n",
       "         [ 0.7483, -1.4027, -0.7539,  0.0272],\n",
       "         [-0.0418, -0.7337, -0.7091, -0.2561],\n",
       "         [ 0.6161, -0.8732, -1.6545,  0.4139],\n",
       "         [ 0.5719, -1.4312, -1.8784,  0.4177],\n",
       "         [ 0.1761, -0.9333, -1.0476,  0.2224],\n",
       "         [ 0.4331, -1.2377, -1.3715,  0.0713],\n",
       "         [ 0.3159, -1.3396, -1.1066, -0.0592],\n",
       "         [ 0.1702, -0.8621, -0.8314, -0.0940],\n",
       "         [ 1.0846, -1.4346, -1.2045,  0.3656],\n",
       "         [ 0.3539, -1.0128, -0.8129, -0.2117],\n",
       "         [ 0.5123, -1.1204, -0.3847, -0.1259],\n",
       "         [ 0.6047, -1.5755, -1.4912,  0.1619],\n",
       "         [ 0.0165, -1.1948, -1.5551,  0.3009],\n",
       "         [ 0.4306, -1.2178, -1.5964,  0.4244],\n",
       "         [ 0.6353, -2.0520, -1.3048, -0.0253],\n",
       "         [ 0.5626, -0.8008, -0.1968, -0.1044],\n",
       "         [ 0.4658, -1.2448, -1.3412,  0.0283],\n",
       "         [ 0.3876, -1.0208, -1.8767,  0.6009],\n",
       "         [ 0.5852, -1.5354, -1.4263,  0.0745],\n",
       "         [ 0.3791, -0.5328, -0.7801, -0.4409],\n",
       "         [ 0.8696, -1.8204, -1.1256,  0.0140],\n",
       "         [ 1.1103, -1.6542, -0.4796, -0.1052],\n",
       "         [ 0.7485, -1.6194, -0.5923, -0.1446],\n",
       "         [ 1.0022, -1.2875, -0.7576,  0.1343],\n",
       "         [ 0.3454, -1.4782, -1.7086,  0.4494],\n",
       "         [ 0.4329, -1.3626, -0.7198, -0.1963],\n",
       "         [ 0.3670, -1.1688, -2.2553,  0.3109],\n",
       "         [-0.0048, -0.8937, -1.1133,  0.0657],\n",
       "         [ 0.9871, -1.8824, -1.3680,  0.3665],\n",
       "         [ 0.5733, -1.2316, -0.9129, -0.2782],\n",
       "         [ 0.4148, -1.0303, -0.6110, -0.0820],\n",
       "         [ 0.6876, -1.8980, -1.1203, -0.0844],\n",
       "         [ 0.6110, -1.4912, -0.9602,  0.0711],\n",
       "         [ 0.5373, -1.1304, -0.7440, -0.1285],\n",
       "         [ 0.3888, -0.8541, -0.6396,  0.0264],\n",
       "         [ 0.9700, -1.8825, -0.8885,  0.0308],\n",
       "         [ 0.3554, -1.2847, -1.1350,  0.2442],\n",
       "         [ 0.8381, -1.1161, -0.3440,  0.1284],\n",
       "         [ 0.0719, -0.9448, -0.7461, -0.3176],\n",
       "         [ 0.2052, -1.0499, -1.3313,  0.1844],\n",
       "         [ 0.2622, -0.7829, -1.3973, -0.3260],\n",
       "         [ 0.5292, -1.2190, -0.9411,  0.0137],\n",
       "         [ 0.7985, -1.4603, -1.1397,  0.1282],\n",
       "         [ 0.4891, -1.1219, -0.5689, -0.1742],\n",
       "         [ 0.7739, -0.7995, -1.3459,  0.3084],\n",
       "         [ 0.5161, -1.3875, -2.1400,  0.2023],\n",
       "         [ 0.5337, -0.6526, -0.0222, -0.2504],\n",
       "         [ 0.1345, -0.8969, -0.8075, -0.3012],\n",
       "         [ 1.0219, -1.3981, -0.7044,  0.1050],\n",
       "         [ 0.7721, -1.5589, -0.4776, -0.1821],\n",
       "         [ 0.1298, -0.8991, -0.6263, -0.1901]], grad_fn=<AddmmBackward0>),\n",
       " tensor([[ 0.0204,  1.2309,  2.1385,  1.1853],\n",
       "         [-0.3768,  0.9830,  1.7687,  1.1070],\n",
       "         [-0.0422,  1.0019,  1.6908,  0.8795],\n",
       "         [ 0.0602,  0.9569,  1.1702,  0.7742],\n",
       "         [ 0.0148,  1.4879,  1.9256,  1.4025],\n",
       "         [ 0.1052,  1.0518,  1.8794,  1.0401],\n",
       "         [-0.4251,  0.9634,  1.9380,  1.1481],\n",
       "         [ 0.1530,  0.6699,  1.1278,  0.7458],\n",
       "         [-0.1570,  1.1862,  2.0738,  1.0210],\n",
       "         [ 0.1240,  1.0255,  1.8458,  0.8792],\n",
       "         [ 0.0969,  1.3937,  1.6177,  0.9239],\n",
       "         [ 0.0701,  1.3350,  2.0861,  1.2743],\n",
       "         [ 0.0906,  1.0450,  2.2706,  0.9311],\n",
       "         [ 0.0506,  1.0448,  1.5191,  0.9380],\n",
       "         [ 0.0293,  0.6258,  1.7922,  0.4098],\n",
       "         [-0.2095,  0.5566,  1.0815,  0.5295],\n",
       "         [-0.0541,  0.6363,  1.6519,  0.4333],\n",
       "         [ 0.0284,  0.8709,  1.3397,  0.8579],\n",
       "         [ 0.2164,  1.2432,  1.0731,  0.8373],\n",
       "         [ 0.1173,  0.8868,  1.1465,  0.7096],\n",
       "         [ 0.1208,  1.4935,  1.7371,  1.3698],\n",
       "         [-0.1635,  0.5569,  1.1559,  0.6762],\n",
       "         [-0.4597,  0.7194,  0.5418,  1.0789],\n",
       "         [ 0.0259,  1.2189,  2.2950,  1.1145],\n",
       "         [-0.2575,  0.4757,  1.4371,  0.8481],\n",
       "         [-0.4031,  0.8579,  0.6496,  0.6836],\n",
       "         [ 0.0570,  0.8253,  2.0274,  0.7292],\n",
       "         [-0.2037,  0.8127,  1.5907,  0.6511],\n",
       "         [ 0.0190,  1.1523,  1.9183,  1.1918],\n",
       "         [-0.1330,  0.4682,  0.9222,  0.3537],\n",
       "         [-0.7427,  0.7372,  1.7870,  1.1127],\n",
       "         [-0.4839,  1.0023,  1.9454,  1.2053],\n",
       "         [-0.4780,  1.1617,  0.4869,  1.3362],\n",
       "         [-0.2227,  0.7291,  1.8759,  0.8972],\n",
       "         [-0.0792,  0.7545,  1.5254,  0.7035],\n",
       "         [-0.1651,  0.5414,  1.2989,  0.5932],\n",
       "         [-0.7251,  1.6185,  1.3108,  1.3426],\n",
       "         [ 0.0739,  0.5151,  1.8061,  0.5839],\n",
       "         [ 0.0706,  1.0141,  1.3365,  0.8947],\n",
       "         [-0.2823,  1.0783,  1.9073,  0.9344],\n",
       "         [-0.4413,  0.8689,  1.0780,  1.3054],\n",
       "         [-0.4221,  1.0096,  1.2339,  1.4133],\n",
       "         [-0.0847,  1.3255,  2.1769,  1.1028],\n",
       "         [ 0.1734,  0.8354,  1.1898,  0.7142],\n",
       "         [-0.1867,  0.6926,  2.1107,  0.9337],\n",
       "         [-0.6195,  0.7976,  1.7361,  1.4456],\n",
       "         [-0.1433,  0.9655,  2.0003,  0.9991],\n",
       "         [ 0.0824,  0.2878,  1.4039,  0.4342],\n",
       "         [-0.0599,  1.3318,  2.3375,  1.2334],\n",
       "         [ 0.2510,  1.4649,  2.1967,  1.3963],\n",
       "         [ 0.1523,  1.2988,  1.8982,  1.1370],\n",
       "         [-0.0313,  1.3500,  1.7772,  1.1742],\n",
       "         [-0.5719,  1.2013,  1.0547,  1.3633],\n",
       "         [-0.0345,  1.0045,  1.6041,  0.7144],\n",
       "         [-0.5649,  0.7896,  1.5246,  1.2472],\n",
       "         [-0.2889,  0.6834,  0.8729,  0.8807],\n",
       "         [-0.3171,  1.5483,  1.7756,  1.3387],\n",
       "         [ 0.0249,  0.8869,  1.6405,  0.7103],\n",
       "         [-0.0157,  0.8226,  1.4351,  0.7806],\n",
       "         [-0.0299,  1.3533,  1.9322,  1.0032],\n",
       "         [ 0.0197,  1.0855,  1.7797,  0.9886],\n",
       "         [ 0.1548,  0.6394,  1.7595,  0.7856],\n",
       "         [-0.3308,  0.8575,  0.7347,  0.6489],\n",
       "         [ 0.1241,  1.4512,  2.3359,  1.2536],\n",
       "         [-0.2912,  1.1861,  1.0668,  1.2161],\n",
       "         [ 0.1166,  1.3206,  1.4503,  0.9215],\n",
       "         [-0.0347,  0.4901,  1.2956,  0.3795],\n",
       "         [-0.3520,  0.7648,  1.2400,  0.8757],\n",
       "         [-0.0595,  0.4640,  0.8160, -0.0190],\n",
       "         [ 0.0701,  0.7365,  1.7055,  0.8721],\n",
       "         [-0.1475,  1.1512,  1.7713,  1.0979],\n",
       "         [-0.0195,  0.9466,  1.4143,  0.7219],\n",
       "         [-0.4374,  0.7727,  2.2736,  1.2555],\n",
       "         [-0.5367,  0.9037,  1.5113,  0.9913],\n",
       "         [ 0.2646,  0.7088,  1.1155,  0.8165],\n",
       "         [ 0.0541,  0.4653,  1.4407,  0.5501],\n",
       "         [ 0.1264,  1.2457,  1.9857,  1.2504],\n",
       "         [ 0.2644,  1.2476,  1.9905,  1.1081],\n",
       "         [-0.0106,  0.5312,  1.3181,  0.7084]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "BCE_loss = nn.BCELoss()\n",
    "\n",
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = F.cross_entropy(x_hat, x, reduction='sum')\n",
    "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss + KLD\n",
    "\n",
    "# from torch.optim import Adam\n",
    "\n",
    "optimizer = SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VAE...\n",
      "\tEpoch 1 complete! \tAverage Loss:  108.44813586504031\n",
      "\tEpoch 2 complete! \tAverage Loss:  108.22938850598457\n",
      "\tEpoch 3 complete! \tAverage Loss:  108.17118815886668\n",
      "\tEpoch 4 complete! \tAverage Loss:  108.16479575328337\n",
      "\tEpoch 5 complete! \tAverage Loss:  108.15182857024364\n",
      "\tEpoch 6 complete! \tAverage Loss:  108.1175280839969\n",
      "\tEpoch 7 complete! \tAverage Loss:  108.20703134781274\n",
      "\tEpoch 8 complete! \tAverage Loss:  108.14195300371219\n",
      "\tEpoch 9 complete! \tAverage Loss:  108.13398116674179\n",
      "\tEpoch 10 complete! \tAverage Loss:  108.11548873705742\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training VAE...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        # x = x.view(x.size(0), x_dim)\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_hat, mean, log_var = model(x)\n",
    "        loss = loss_function(x, x_hat, mean, log_var)\n",
    "        \n",
    "        overall_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"\\tEpoch\", epoch + 1, \"complete!\", \"\\tAverage Loss: \", overall_loss / batch_idx)\n",
    "    \n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, x in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "        \n",
    "        x_hat = model(x)\n",
    "\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 0., 0., 1., 1., 3., 0., 2., 3., 2., 1., 4., 3., 2., 4., 2.])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy' has no attribute 'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Set random seed for reproducibility\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39mseed(\u001b[39m10\u001b[39m)\n\u001b[1;32m      6\u001b[0m in_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m6\u001b[39m,(\u001b[39m100\u001b[39m,\u001b[39m50\u001b[39m))\n\u001b[1;32m      7\u001b[0m in_data\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jax.numpy' has no attribute 'random'"
     ]
    }
   ],
   "source": [
    "import jax.numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(10)\n",
    "in_data = np.random.randint(0,6,(100,50))\n",
    "in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5, 4, ..., 0, 3, 4],\n",
       "       [2, 0, 1, ..., 1, 5, 0],\n",
       "       [2, 3, 5, ..., 3, 1, 3],\n",
       "       ...,\n",
       "       [3, 3, 2, ..., 5, 3, 3],\n",
       "       [0, 3, 4, ..., 2, 5, 4],\n",
       "       [2, 0, 2, ..., 4, 2, 1]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = np.array(in_data[81:])\n",
    "train_input = np.array(in_data[0:80])\n",
    "train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_input = torch.tensor(input_data[81:]).float()\n",
    "# train_input = torch.tensor(input_data[0:80]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements auto-encoding variational Bayes (variational autoencoder).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division\n",
    "from __future__ import print_function\n",
    "import jax.random as random\n",
    "import numpy as onp\n",
    "from jax.scipy.stats import norm\n",
    "from jax.nn import sigmoid\n",
    "from jax import vmap, grad, value_and_grad, jit, tree_util\n",
    "from jax.example_libraries.optimizers import adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "def diag_gaussian_log_density(x, mu, log_sigma):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: random variable\n",
    "    mu: mean\n",
    "    log_sigma: log standard deviation\n",
    "  Return:\n",
    "    log normal density.\n",
    "  \"\"\"\n",
    "  assert x.ndim == 1\n",
    "  return np.sum(norm.logpdf(x, mu, np.exp(log_sigma)), axis=-1)\n",
    "\n",
    "\n",
    "def unpack_gaussian_params(params):\n",
    "  \"\"\"\n",
    "  Args: \n",
    "    params of a diagonal Gaussian.\n",
    "  Return:\n",
    "    mean, log standard deviation\n",
    "  \"\"\"\n",
    "  D = np.shape(params)[-1] // 2\n",
    "  print(\"params shape\", params.shape)\n",
    "  mu, log_sigma = params[:D], params[D:]\n",
    "  return mu, log_sigma\n",
    "\n",
    "\n",
    "def sample_diag_gaussian(mu, log_std, subkey):\n",
    "  \"\"\"Reparameterization trick for getting z from x.\n",
    "  \"\"\"\n",
    "  return random.normal(subkey, mu.shape) * np.exp(log_std) + mu\n",
    "\n",
    "\n",
    "def gaussian_log_density(x, mu, logvar):\n",
    "  \"\"\"\n",
    "  Args: \n",
    "    x: input value\n",
    "    mu: mean of the Gaussian distribution\n",
    "    logvar: log variance of the Gaussian distribution\n",
    "  Return: \n",
    "    log N(x | mu, var)\n",
    "  \"\"\"\n",
    "  c = -0.5 * math.log(2 * math.pi)\n",
    "  return c - 0.5 * logvar - (x - mu) ** 2 / (2 * torch.exp(logvar))\n",
    "\n",
    "\n",
    "def init_net_params(scale, layer_sizes, key):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    scale: scaling factor\n",
    "    layer_sizes: List[number of neurons per layer]\n",
    "  Return:\n",
    "    Tuple[weights, biases] for all layers.\"\"\"\n",
    "  k1, k2 = random.split(key, 2)\n",
    "  return [\n",
    "      (\n",
    "          scale * random.normal(k1, (m, n)),  # weight matrix\n",
    "          scale * random.normal(k2, (n,)))  # bias vector\n",
    "      for m, n in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "  ]\n",
    "\n",
    "\n",
    "def neural_net_predict(params, inputs):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    params: List[Tuple(weights, bias)]\n",
    "    inputs: an (N x D) matrix, (batch 2D latent vector Dz x B), here Dz = 2\n",
    "  Return:\n",
    "    Applies batch normalization to every layer but the last.\"\"\"\n",
    "  for W, b in params:\n",
    "    print(f\"W {W.shape} b {b.shape}\")\n",
    "    if W.shape[0] == inputs.shape[0]:\n",
    "      outputs = np.dot(inputs.T, W) + b\n",
    "    else:\n",
    "      outputs = np.dot(inputs, W) + b  # linear transformation\n",
    "    inputs = np.tanh(outputs)  # nonlinear transformation\n",
    "  return outputs\n",
    "\n",
    "\n",
    "def nn_predict_gaussian(params, inputs):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    params: variational parameters\n",
    "    inputs: batch of datas\n",
    "  Return:\n",
    "    means and diagonal variances\n",
    "  \"\"\"\n",
    "  return unpack_gaussian_params(neural_net_predict(params, inputs))\n",
    "\n",
    "\n",
    "def log_prior(z):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    z: latent variable\n",
    "  Return:\n",
    "    computes log of pior over digit's latent represenation z.\n",
    "  \"\"\"\n",
    "  assert z.ndim == 1\n",
    "  return diag_gaussian_log_density(z, 0, 0)\n",
    "\n",
    "\n",
    "def decoder(z, params):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    z: latent representation\n",
    "    params: decoder network parameters, theta\n",
    "  Return: \n",
    "    784-D (28x28 pixels) mean vector of prod of Bern\n",
    "  \"\"\"\n",
    "  logits = neural_net_predict(params, z)\n",
    "  return logits\n",
    "\n",
    "\n",
    "def log_likelihood(z, x, params):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    z: latent representation\n",
    "    x: binarized digit\n",
    "    params: logits from decoder/generator theta\n",
    "  Return: \n",
    "    log likelihood log p(x|z), p data given latent\n",
    "  \"\"\"\n",
    "  mu = decoder(z, params)  # logits\n",
    "  likelihood = gaussian_log_density(x, mu)\n",
    "  assert likelihood.ndim == 1\n",
    "  return np.sum(likelihood) # sum over pixels\n",
    "\n",
    "\n",
    "def generate_from_prior(gen_params,\n",
    "                        num_samples,\n",
    "                        noise_dim,\n",
    "                        key=random.PRNGKey(2)):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    gen_params: decoder parameters\n",
    "    num_samples: number of latent variable samples\n",
    "  Return: \n",
    "    Fake data: Bernouilli means p(x|z)\n",
    "  \"\"\"\n",
    "  latents = random.normal(key, (num_samples, noise_dim))\n",
    "  return sigmoid(neural_net_predict(gen_params, latents))\n",
    "\n",
    "\n",
    "def joint_log_density(x, z, params):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "      z: latent representation\n",
    "      x: binarized digit\n",
    "    params: logits from decoder\n",
    "  Return: \n",
    "    log p(z, x) for a single data\n",
    "  \"\"\"\n",
    "  return log_prior(z) + log_likelihood(z, x, params)\n",
    "\n",
    "\n",
    "def encoder(x, params):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: batch of datas\n",
    "    params: variational parameters mu and sigma\n",
    "    phi: recognition parameters\n",
    "  Return: \n",
    "    mean and log std of factorized Gaussian with D = 2\n",
    "  \"\"\"\n",
    "  mu, log_sigma = nn_predict_gaussian(params, x)\n",
    "  return mu, log_sigma\n",
    "\n",
    "\n",
    "def log_q(z, mu, log_sigma):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    z: latent representation\n",
    "    mu, log_sigma: variational distribution parameters\n",
    "  Return: \n",
    "    p(x|params) likelihood of x\n",
    "  \"\"\"\n",
    "  return diag_gaussian_log_density(z, mu, log_sigma)\n",
    "\n",
    "\n",
    "def elbo(x, params, subkey):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: batch of B datas, D_x x B\n",
    "      only need to sample single z for each data in the batch\n",
    "    params: {encoder (recognition network): encoder_params phi,\n",
    "             decoder (likelihood): decoder_params theta}\n",
    "    subkey: jax random key\n",
    "  Return: \n",
    "    scalar, unbiased estimate of mean variaitonal elbo on datas\n",
    "  \"\"\"\n",
    "  encoder_params, decoder_params = params['enc'], params['dec']\n",
    "  # latent means and log stds\n",
    "  mu_qz, log_sigma_qz = encoder(x, encoder_params) \n",
    "  # Monte Carlo est of KL divergence of q from prior p (both Gaussian)\n",
    "  # KL(q(z | x) || p(z)),  q ~ N(z | mu(x), sigma(x)) and p ~ N(0, I_DzxDz)\n",
    "  kl = -1 / 2 * np.sum(\n",
    "      np.log(np.square(np.exp(log_sigma_qz))) + 1 -\n",
    "      np.square(np.exp(log_sigma_qz)) - np.square(mu_qz))\n",
    "  # latent variables\n",
    "  z = sample_diag_gaussian(mu_qz, log_sigma_qz, subkey)\n",
    "  # p(data x | latents z)\n",
    "  ll = log_likelihood(z, x, decoder_params)\n",
    "\n",
    "  return ll - kl\n",
    "\n",
    "\n",
    "def loss(*args, **kwargs):\n",
    "  # Note: negate ll for the elbo loss to minimize\n",
    "  return -elbo(*args, **kwargs)\n",
    "\n",
    "\n",
    "def batch_loss(*args, **kwargs):\n",
    "  \"\"\"Negative elbo estimate over batch of data.\"\"\"\n",
    "  loss_ = vmap(\n",
    "      loss, in_axes=(0, None, 0))(*args,\n",
    "                                  **kwargs)  # correspond each sample with input\n",
    "  return np.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyper-parameters\n",
    "latent_dim = 4\n",
    "# data_dim = train_input.size(0)*test_input.size(1)  # How many pixels in each data (28x28).\n",
    "data_dim = 4000\n",
    "gen_layer_sizes = [latent_dim, 20, data_dim]  # decoder has 200 hidden\n",
    "rec_layer_sizes = [data_dim, 20, latent_dim * 2]  # encoder has 200 hidden\n",
    "\n",
    "# Training parameters\n",
    "param_scale = 0.01\n",
    "# batch_size = batch_size\n",
    "num_epochs = 10  # train for 100 epochs\n",
    "learning_rate = 0.1\n",
    "\n",
    "key = random.PRNGKey(seed)\n",
    "key, enc_k, dec_k = random.split(key, 3)\n",
    "init_gen_params = init_net_params(param_scale, gen_layer_sizes,\n",
    "                                dec_k)  # encoder\n",
    "init_rec_params = init_net_params(param_scale, rec_layer_sizes,\n",
    "                                enc_k)  # decoder\n",
    "combined_init_params = dict(dec=init_gen_params, enc=init_rec_params)\n",
    "\n",
    "num_batches = int(np.ceil(len(train_input) / batch_size))\n",
    "\n",
    "def batch_indices(iter):\n",
    "    idx = iter % num_batches\n",
    "    return slice(idx * batch_size, (idx + 1) * batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_grad = jit(value_and_grad(batch_loss,\n",
    "                                      argnums=1))  # differentiate w.r.t params\n",
    "\n",
    "opt_init, opt_update, opt_get_params = adam(step_size=learning_rate)\n",
    "opt_state = opt_init(combined_init_params)\n",
    "\n",
    "it = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subkeys shape:  (80, 2)\n",
      "W (4000, 20) b (20,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TracerArrayConversionError",
     "evalue": "The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int32[50])>with<BatchTrace(level=4/0)> with\n  val = Traced<ShapedArray(int32[80,50])>with<DynamicJaxprTrace(level=1/0)>\n  batch_dim = 0\nThis BatchTracer with object id 140171901179264 was created on line:\n  /tmp/ipykernel_128713/1397141302.py:233 (batch_loss)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m subkeys \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(subkeys, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msubkeys shape: \u001b[39m\u001b[39m\"\u001b[39m, subkeys\u001b[39m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m loss_, grad_ \u001b[39m=\u001b[39m objective_grad(batch_x, params, subkeys)\n\u001b[1;32m      9\u001b[0m opt_state \u001b[39m=\u001b[39m opt_update(it, grad_, opt_state)\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m it \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# save samples during training\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 19 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[99], line 233\u001b[0m, in \u001b[0;36mbatch_loss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_loss\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    232\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Negative elbo estimate over batch of data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m   loss_ \u001b[39m=\u001b[39m vmap(\n\u001b[1;32m    234\u001b[0m       loss, in_axes\u001b[39m=\u001b[39;49m(\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m, \u001b[39m0\u001b[39;49m))(\u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    235\u001b[0m                                   \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# correspond each sample with input\u001b[39;00m\n\u001b[1;32m    236\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(loss_)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[99], line 228\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    227\u001b[0m   \u001b[39m# Note: negate ll for the elbo loss to minimize\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39melbo(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[99], line 212\u001b[0m, in \u001b[0;36melbo\u001b[0;34m(x, params, subkey)\u001b[0m\n\u001b[1;32m    210\u001b[0m encoder_params, decoder_params \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39menc\u001b[39m\u001b[39m'\u001b[39m], params[\u001b[39m'\u001b[39m\u001b[39mdec\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    211\u001b[0m \u001b[39m# latent means and log stds\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m mu_qz, log_sigma_qz \u001b[39m=\u001b[39m encoder(x, encoder_params) \n\u001b[1;32m    213\u001b[0m \u001b[39m# Monte Carlo est of KL divergence of q from prior p (both Gaussian)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39m# KL(q(z | x) || p(z)),  q ~ N(z | mu(x), sigma(x)) and p ~ N(0, I_DzxDz)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m kl \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(\n\u001b[1;32m    216\u001b[0m     np\u001b[39m.\u001b[39mlog(np\u001b[39m.\u001b[39msquare(np\u001b[39m.\u001b[39mexp(log_sigma_qz))) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m\n\u001b[1;32m    217\u001b[0m     np\u001b[39m.\u001b[39msquare(np\u001b[39m.\u001b[39mexp(log_sigma_qz)) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39msquare(mu_qz))\n",
      "Cell \u001b[0;32mIn[99], line 184\u001b[0m, in \u001b[0;36mencoder\u001b[0;34m(x, params)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencoder\u001b[39m(x, params):\n\u001b[1;32m    176\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m    x: batch of datas\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m    mean and log std of factorized Gaussian with D = 2\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m   mu, log_sigma \u001b[39m=\u001b[39m nn_predict_gaussian(params, x)\n\u001b[1;32m    185\u001b[0m   \u001b[39mreturn\u001b[39;00m mu, log_sigma\n",
      "Cell \u001b[0;32mIn[99], line 107\u001b[0m, in \u001b[0;36mnn_predict_gaussian\u001b[0;34m(params, inputs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnn_predict_gaussian\u001b[39m(params, inputs):\n\u001b[1;32m    100\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m    params: variational parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39m    means and diagonal variances\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m   \u001b[39mreturn\u001b[39;00m unpack_gaussian_params(neural_net_predict(params, inputs))\n",
      "Cell \u001b[0;32mIn[99], line 94\u001b[0m, in \u001b[0;36mneural_net_predict\u001b[0;34m(params, inputs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(inputs\u001b[39m.\u001b[39mT, W) \u001b[39m+\u001b[39m b\n\u001b[1;32m     93\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     outputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(inputs, W) \u001b[39m+\u001b[39m b  \u001b[39m# linear transformation\u001b[39;00m\n\u001b[1;32m     95\u001b[0m   inputs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(outputs)  \u001b[39m# nonlinear transformation\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/jax/_src/core.py:557\u001b[0m, in \u001b[0;36mTracer.__array__\u001b[0;34m(self, *args, **kw)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[0;32m--> 557\u001b[0m   \u001b[39mraise\u001b[39;00m TracerArrayConversionError(\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mTracerArrayConversionError\u001b[0m: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced<ShapedArray(int32[50])>with<BatchTrace(level=4/0)> with\n  val = Traced<ShapedArray(int32[80,50])>with<DynamicJaxprTrace(level=1/0)>\n  batch_dim = 0\nThis BatchTracer with object id 140171901179264 was created on line:\n  /tmp/ipykernel_128713/1397141302.py:233 (batch_loss)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for batch in tqdm(range(num_batches)):\n",
    "        batch_x = train_input[batch_indices(batch)]\n",
    "        params = opt_get_params(opt_state)\n",
    "        key, *subkeys = random.split(key, batch_size + 1)\n",
    "        subkeys = np.stack(subkeys, axis=0)\n",
    "        print(\"subkeys shape: \", subkeys.shape)\n",
    "        loss_, grad_ = objective_grad(batch_x, params, subkeys)\n",
    "        opt_state = opt_update(it, grad_, opt_state)\n",
    "        \n",
    "        if it % 100 == 0: # save samples during training\n",
    "            gen_params, rec_params = params['dec'], params['enc']\n",
    "            fake_data = generate_from_prior(gen_params, 20, latent_dim, key)\n",
    "            save_data(fake_data, 'vae_samples.png', vmin=0, vmax=1)\n",
    "        \n",
    "        if it == 0 or (it + 1) % 100 == 0:\n",
    "            test_size = test_input.shape[0]\n",
    "            print(\"test size: \", test_input.shape, train_input.shape)\n",
    "            key, *subkeys = random.split(key, test_size + 1)\n",
    "            subkeys = np.stack(subkeys, axis=0)\n",
    "            # print performance\n",
    "            loss_t = batch_loss(test_input, params, subkeys)\n",
    "            message = f\"Epoch: {epoch} \\t Batch: {batch} \\t Loss: {loss_:.3f} \\t Test Loss: {loss_t:.3f}\"\n",
    "            tqdm.write(message)\n",
    "        it += 1\n",
    "\n",
    "# pickle to save trained weights\n",
    "params = opt_get_params(opt_state)\n",
    "with open(param_dump, 'wb') as file:\n",
    "    pickle.dump(   params, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## TRAINING VAE ##############\n",
    "\n",
    "def train(train_data, test_data, param_dump='opt-params.pkl', seed=0):\n",
    "  \"\"\"\n",
    "  Optimize gradients of weights over batches of data with elbo estimate.\n",
    "  \"\"\"\n",
    "  # Model hyper-parameters\n",
    "  latent_dim = 4\n",
    "  data_dim = train_input.size(0)*test_input.size(1)  # How many pixels in each data (28x28).\n",
    "  gen_layer_sizes = [latent_dim, 200, data_dim]  # decoder has 200 hidden\n",
    "  rec_layer_sizes = [data_dim, 200, latent_dim * 2]  # encoder has 200 hidden\n",
    "\n",
    "  # Training parameters\n",
    "  param_scale = 0.01\n",
    "  # batch_size = batch_size\n",
    "  num_epochs = 10  # train for 100 epochs\n",
    "  learning_rate = 0.1\n",
    "\n",
    "  key = random.PRNGKey(seed)\n",
    "  key, enc_k, dec_k = random.split(key, 3)\n",
    "  init_gen_params = init_net_params(param_scale, gen_layer_sizes,\n",
    "                                    dec_k)  # encoder\n",
    "  init_rec_params = init_net_params(param_scale, rec_layer_sizes,\n",
    "                                    enc_k)  # decoder\n",
    "  combined_init_params = dict(dec=init_gen_params, enc=init_rec_params)\n",
    "\n",
    "  num_batches = int(np.ceil(len(train_data) / batch_size))\n",
    "\n",
    "  def batch_indices(iter):\n",
    "    idx = iter % num_batches\n",
    "    return slice(idx * batch_size, (idx + 1) * batch_size)\n",
    "\n",
    "  objective_grad = jit(value_and_grad(batch_loss,\n",
    "                                      argnums=1))  # differentiate w.r.t params\n",
    "\n",
    "  opt_init, opt_update, opt_get_params = adam(step_size=learning_rate)\n",
    "  opt_state = opt_init(combined_init_params)\n",
    "\n",
    "  it = 0\n",
    "  for epoch in tqdm(range(num_epochs)):\n",
    "    for batch in tqdm(range(num_batches)):\n",
    "      batch_x = train_data[batch_indices(batch)]\n",
    "      params = opt_get_params(opt_state)\n",
    "      key, *subkeys = random.split(key, batch_size + 1)\n",
    "      subkeys = np.stack(subkeys, axis=0)\n",
    "      print(\"subkeys shape: \", subkeys.shape)\n",
    "      loss_, grad_ = objective_grad(batch_x, params, subkeys)\n",
    "      opt_state = opt_update(it, grad_, opt_state)\n",
    "\n",
    "      if it % 100 == 0: # save samples during training\n",
    "        gen_params, rec_params = params['dec'], params['enc']\n",
    "        fake_data = generate_from_prior(gen_params, 20, latent_dim, key)\n",
    "        save_data(fake_data, 'vae_samples.png', vmin=0, vmax=1)\n",
    "\n",
    "      if it == 0 or (it + 1) % 100 == 0:\n",
    "        test_size = test_data.shape[0]\n",
    "        print(\"test size: \", test_data.shape, train_data.shape)\n",
    "        key, *subkeys = random.split(key, test_size + 1)\n",
    "        subkeys = np.stack(subkeys, axis=0)\n",
    "        # print performance\n",
    "        loss_t = batch_loss(test_data, params, subkeys)\n",
    "        message = f\"Epoch: {epoch} \\t Batch: {batch} \\t Loss: {loss_:.3f} \\t Test Loss: {loss_t:.3f}\"\n",
    "        tqdm.write(message)\n",
    "      it += 1\n",
    "\n",
    "  # pickle to save trained weights\n",
    "  params = opt_get_params(opt_state)\n",
    "  with open(param_dump, 'wb') as file:\n",
    "    pickle.dump(params, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subkeys shape:  (10, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dtype torch.float32 not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/jax/_src/api_util.py:556\u001b[0m, in \u001b[0;36mshaped_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m   \u001b[39mreturn\u001b[39;00m _shaped_abstractify_handlers[\u001b[39mtype\u001b[39;49m(x)](x)\n\u001b[1;32m    557\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: <class 'torch.Tensor'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/jax/_src/dtypes.py:110\u001b[0m, in \u001b[0;36m_canonicalize_dtype\u001b[0;34m(x64_enabled, allow_opaque_dtype, dtype)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m   dtype_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdtype(dtype)\n\u001b[1;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'torch.float32' as a data type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(train_input, test_input, param_dump\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mopt-params.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[29], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_data, test_data, param_dump, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m subkeys \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(subkeys, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msubkeys shape: \u001b[39m\u001b[39m\"\u001b[39m, subkeys\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 47\u001b[0m loss_, grad_ \u001b[39m=\u001b[39m objective_grad(batch_x, params, subkeys)\n\u001b[1;32m     48\u001b[0m opt_state \u001b[39m=\u001b[39m opt_update(it, grad_, opt_state)\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m it \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# save samples during training\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/jax/_src/dtypes.py:112\u001b[0m, in \u001b[0;36m_canonicalize_dtype\u001b[0;34m(x64_enabled, allow_opaque_dtype, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m   dtype_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdtype(dtype)\n\u001b[1;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 112\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdtype \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m!r}\u001b[39;00m\u001b[39m not understood\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m x64_enabled:\n\u001b[1;32m    115\u001b[0m   \u001b[39mreturn\u001b[39;00m dtype_\n",
      "\u001b[0;31mTypeError\u001b[0m: dtype torch.float32 not understood"
     ]
    }
   ],
   "source": [
    "train(train_input, test_input, param_dump='opt-params.pkl', seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## VISUALIZE APPROXIMATE POSTERIOR #############\n",
    "\n",
    "def load_params(file='params2.pkl'):\n",
    "  with open(file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "  # JAX does not recognize pickled file, must re-format\n",
    "  # params: List[[Tuple(weights), Tuple(bias)]]\n",
    "  num_layers = 2\n",
    "  for k in ['dec', 'enc']:\n",
    "    params[k] = list(params[k])\n",
    "    for l in range(num_layers):\n",
    "      params[k][l] = tuple(params[k][l])\n",
    "\n",
    "  print(\"after loaded params\", type(params), type(params['enc']),\n",
    "        type(params['dec'][0]), type(params['dec'][0][0]))\n",
    "\n",
    "  return params\n",
    "\n",
    "\n",
    "def sample_gen(params, num_samples=10, seed=0):\n",
    "  \"\"\"\n",
    "  Args: \n",
    "    params: the variational parameters\n",
    "    num_samples: number of times to sample from distributino\n",
    "    seed: random seed\n",
    "  Plot samples from trained generative model using ancestral sampling.\n",
    "  \"\"\"\n",
    "  key = random.PRNGKey(seed)\n",
    "  key, k1, k2 = random.split(key, 3)\n",
    "  # sample z from prior num_samples times\n",
    "  # use generative model to compute bernouilli means over pixels of x given z\n",
    "  means = generate_from_prior(params['dec'], num_samples, 2, k1)\n",
    "  # plot means as greyscale data\n",
    "  mean_data = means.reshape([-1, 28, 28])\n",
    "  # sample binary data x from product of Bern and plot as data\n",
    "  sample_means = random.gaussian(k2, mean_data)\n",
    "  # concatenate plots: row 1, bernouilli means, row 2 corresponding binary img sampled from 1\n",
    "  plot_means = np.stack([mean_data, sample_means])\n",
    "  data_ = onp.zeros([2 * 28, 10 * 28])\n",
    "  num_rows = 2\n",
    "  num_cols = 10\n",
    "  for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "      data_[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = plot_means[i, j, ...]\n",
    "  plt.imshow(data_, cmap=plt.cm.binary)\n",
    "  plt.axis('off')\n",
    "  plt.savefig('gen_samples.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "def latent_means(params, train_data, train_labels):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    params: List[Tuple(W, b)] for each layer in NN\n",
    "    train_data, train_labels = (10k, 784), (10k, 10)\n",
    "  Latent space scatter plot, each point is a different data in training set.\n",
    "  Visualizes which part of latent space corresponds to which kinds of data.\n",
    "  \"\"\"\n",
    "  # encode each data in the train set\n",
    "  mus, log_sigmas = vmap(\n",
    "      encoder, in_axes=(0, None))(train_data, params['enc'])\n",
    "  # one hot encode -> continuous\n",
    "  labels = np.argmax(train_labels, axis=-1)\n",
    "  num_labels = train_labels.shape[-1]\n",
    "\n",
    "  # 2D mean vector of each encoding q_phi(z|x)\n",
    "  # plot mean vectors in 2D latent space\n",
    "  # color each point to class label (0, 9)\n",
    "  def cmap_process(cmap, N):\n",
    "    if type(cmap) == str:\n",
    "      cmap = plt.get_cmap(cmap)\n",
    "    col_idx = onp.concatenate((onp.linspace(0, 1., N), (0., 0., 0., 0.)))\n",
    "    col_rgb = cmap(col_idx)\n",
    "    idx = onp.linspace(0, 1., N + 1)\n",
    "    cols = {}\n",
    "    for k_i, k in enumerate(('red', 'green', 'blue')):\n",
    "      cols[k] = [\n",
    "          (idx[i], col_rgb[i - 1, k_i], col_rgb[i, k_i]) for i in range(N + 1)\n",
    "      ]\n",
    "\n",
    "    return mpl.colors.LinearSegmentedColormap(f\"{cmap.name}-{N}\", cols, 1024)\n",
    "\n",
    "  def color_index(num_colors, cmap):\n",
    "    cmap = cmap_process(cmap, num_colors)\n",
    "    color_map = mpl.cm.ScalarMappable(cmap=cmap)\n",
    "    color_map.set_array([])\n",
    "    color_map.set_clim(-0.5, num_colors + 0.5)\n",
    "    color_bar = plt.colorbar(color_map, fraction=0.045, pad=0.04)\n",
    "    color_bar.set_ticks(onp.linspace(0, num_colors, num_colors))\n",
    "    color_bar.set_ticklabels(range(num_colors))\n",
    "\n",
    "    return color_bar\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  cmap = plt.cm.jet\n",
    "  ax.scatter(mus[:, 0], mus[:, 1], c=labels, s=1, cmap=cmap)\n",
    "  cb = color_index(num_labels, cmap)\n",
    "  ratio = 1.0\n",
    "  left, right = ax.get_xlim()\n",
    "  low, hi = ax.get_ylim()\n",
    "  ax.set_aspect(abs((right - left) / (low - hi)) * ratio)\n",
    "  ax.set_xlabel(r'$\\mu_z(x)_0$')\n",
    "  ax.set_ylabel(r'$\\mu_z(x)_1$')\n",
    "  ax.set_title(\"Latent posterior mean given data\")\n",
    "  fig.set_size_inches([6, 6], forward=True)\n",
    "  plt.savefig(\"latent_posterior.png\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "# def lin_interpolate(params, train_data, train_labels, examples):\n",
    "#   \"\"\"\n",
    "#   Args:\n",
    "#     params: List[Tuple(W, b)] for each layer in NN\n",
    "#     train_data, train_labels = (10k, 784), (10k, 10)\n",
    "#     examples: List[Tuple[digit 1, digit 2]] samples to interpolate\n",
    "#   Examining latent variable model with continuous latent variables by \n",
    "#   linearly interpolating between latent reps (mean vecs of encodings) of two points.\n",
    "#   \"\"\"\n",
    "\n",
    "#   def interpolate(za, zb, alpha):\n",
    "#     \"\"\"Linear interpolation z_alpha = alpha * z_a + (1-a) * z_b\n",
    "#     \"\"\"\n",
    "#     z_alpha = alpha * za + (1 - alpha) * zb\n",
    "#     return z_alpha\n",
    "\n",
    "#   # sample 3 pairs of datas, each having a different class\n",
    "#   labels_to_data = defaultdict(list)\n",
    "#   # encode data and get mean vectors\n",
    "#   labels = np.argmax(train_labels, axis=-1)\n",
    "#   # linearly interpolate between mean vectors\n",
    "#   for im, lab in tqdm(zip(train_data, labels)):\n",
    "#     labels_to_data[lab].append(im)\n",
    "#   print(\"labels to datas\", labels_to_data.keys())\n",
    "#   # plot Bernoulli means p(x|z_\\alpha) at 10 equally spaced points\n",
    "#   data_ = onp.zeros([3 * 28, 10 * 28])\n",
    "#   # plot generative distribution along linear interpolation\n",
    "#   for row, pair in enumerate(examples):\n",
    "#     datas = [labels_to_data[pair[0]][0], labels_to_data[pair[1]][0]]\n",
    "#     datas = np.stack(datas)\n",
    "#     mus, log_sigmas = vmap(encoder, in_axes=(0, None))(datas, params['enc'])\n",
    "#     alphas = np.linspace(0, 1, 10)[::-1]\n",
    "#     interpolated_means = [interpolate(mus[0], mus[1], a) for a in alphas]\n",
    "#     interpolated_means = np.stack(interpolated_means)\n",
    "#     bern_mus = sigmoid(\n",
    "#         vmap(decoder, in_axes=(0, None))(interpolated_means, params['dec']))\n",
    "#     bern_ims = bern_mus.reshape([-1, 28, 28])\n",
    "#     print(\"bern ims\", bern_ims.shape)\n",
    "#     for col in range(10):\n",
    "#       data_[row * 28:(row + 1) * 28, col * 28:(col + 1) *\n",
    "#              28] = bern_ims[col, ...]\n",
    "\n",
    "#   fig, ax = plt.subplots()\n",
    "#   plt.imshow(data_, cmap=plt.cm.binary)\n",
    "#   plt.axis('off')\n",
    "#   plt.savefig('interpolated_means.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subkeys shape:  (10, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dtype torch.float32 not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/jax/_src/api_util.py:556\u001b[0m, in \u001b[0;36mshaped_abstractify\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m   \u001b[39mreturn\u001b[39;00m _shaped_abstractify_handlers[\u001b[39mtype\u001b[39;49m(x)](x)\n\u001b[1;32m    557\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: <class 'torch.Tensor'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/jax/_src/dtypes.py:110\u001b[0m, in \u001b[0;36m_canonicalize_dtype\u001b[0;34m(x64_enabled, allow_opaque_dtype, dtype)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 110\u001b[0m   dtype_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdtype(dtype)\n\u001b[1;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret 'torch.float32' as a data type",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 195\u001b[0m\n\u001b[1;32m    193\u001b[0m seed \u001b[39m=\u001b[39m \u001b[39m412\u001b[39m\n\u001b[1;32m    194\u001b[0m num_samples \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m--> 195\u001b[0m train(train_data, test_data, \u001b[39m'\u001b[39;49m\u001b[39mparams.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, seed)\n\u001b[1;32m    197\u001b[0m \u001b[39m# plot samples form generative model\u001b[39;00m\n\u001b[1;32m    198\u001b[0m opt_params \u001b[39m=\u001b[39m load_params(\u001b[39m'\u001b[39m\u001b[39mparams.pkl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_data, test_data, param_dump, seed)\u001b[0m\n\u001b[1;32m     45\u001b[0m subkeys \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(subkeys, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msubkeys shape: \u001b[39m\u001b[39m\"\u001b[39m, subkeys\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 47\u001b[0m loss_, grad_ \u001b[39m=\u001b[39m objective_grad(batch_x, params, subkeys)\n\u001b[1;32m     48\u001b[0m opt_state \u001b[39m=\u001b[39m opt_update(it, grad_, opt_state)\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m it \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39m# save samples during training\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/glue_env/lib/python3.10/site-packages/jax/_src/dtypes.py:112\u001b[0m, in \u001b[0;36m_canonicalize_dtype\u001b[0;34m(x64_enabled, allow_opaque_dtype, dtype)\u001b[0m\n\u001b[1;32m    110\u001b[0m   dtype_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdtype(dtype)\n\u001b[1;32m    111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 112\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdtype \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m!r}\u001b[39;00m\u001b[39m not understood\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m x64_enabled:\n\u001b[1;32m    115\u001b[0m   \u001b[39mreturn\u001b[39;00m dtype_\n",
      "\u001b[0;31mTypeError\u001b[0m: dtype torch.float32 not understood"
     ]
    }
   ],
   "source": [
    "############ STOCHASTIC VARIATIONAL INFERENCE #############\n",
    "\n",
    "def top_half(x):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: data\n",
    "  Return: \n",
    "    top half of 28x28 data array.\n",
    "  \"\"\"\n",
    "  assert x.shape == (28, 28)\n",
    "  return x[:14, :]\n",
    "\n",
    "\n",
    "def log_like_top_half(x, z, params):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    z: latent vector\n",
    "    x: data\n",
    "    params: decoder parameters\n",
    "  Return: \n",
    "    log p(top half of data x | z) integrated out exactly for\n",
    "      all unobserved dimensions of x are leaf nodes since ll factorizes\n",
    "  \"\"\"\n",
    "  x = x.reshape([28, 28])\n",
    "  mu_logits = decoder(z, params)  # unnormalized_logprob\n",
    "  mu_data = mu_logits.reshape([28, 28])\n",
    "  data_top_half = top_half(x)\n",
    "  mu_top_half = top_half(mu_data)\n",
    "  gaus_density = gaussian_log_density(data_top_half, mu_top_half)\n",
    "  return np.sum(gaus_density)\n",
    "\n",
    "\n",
    "def joint_ll_top_half(x, zs, params):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: data\n",
    "    zs: array\n",
    "    params; decoder parameters\n",
    "  Return: \n",
    "    log joint density log p(z, top half data x) for each z\n",
    "  \"\"\"\n",
    "  return log_prior(zs) + log_like_top_half(x, zs, params)\n",
    "\n",
    "\n",
    "def init_var_params(subkey):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    subkey: jax key\n",
    "  Return:\n",
    "    Initialized variational parameters phi_mu and phi_logsigma for\n",
    "    variational distribution q(z|top half of x).\n",
    "  \"\"\"\n",
    "  return random.normal(subkey, (4,))\n",
    "\n",
    "\n",
    "@jit\n",
    "def elbo_half(*args, **kwargs):\n",
    "  \"\"\"\n",
    "  ELBO estimate over K samples, batched for half of data x.\n",
    "  \"\"\"\n",
    "\n",
    "  def elbo_k(x, qz_params, dec_params, subkey):\n",
    "    \"\"\"\n",
    "    Estimate of ELBO over K samples z ~ q(z | top half of x).\n",
    "    \"\"\"\n",
    "    mu_qz, log_sigma_qz = unpack_gaussian_params(qz_params)\n",
    "    kl = -1 / 2 * np.sum(\n",
    "        np.log(np.square(np.exp(log_sigma_qz))) + 1 -\n",
    "        np.square(np.exp(log_sigma_qz)) - np.square(mu_qz))\n",
    "    z = sample_diag_gaussian(mu_qz, log_sigma_qz, subkey)\n",
    "    ll = log_like_top_half(x, z, dec_params)\n",
    "    return ll - kl\n",
    "\n",
    "  loss_ = vmap(elbo_k, in_axes=(None, None, None, 0))(*args, *kwargs)\n",
    "  return np.mean(loss_)\n",
    "\n",
    "\n",
    "def optimize_params(params, train_data, seed):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    params: variational and generator model parameters\n",
    "    train_data: single digit from training datas.\n",
    "  Return:\n",
    "    Optimized phi_mu and phi_logsigma for one digit from set.\n",
    "  \"\"\"\n",
    "  key = random.PRNGKey(seed)\n",
    "  key, subkey = random.split(key)\n",
    "  qz_params = init_var_params(subkey)\n",
    "  grad_elbo = jit(grad(elbo_half, argnums=1))\n",
    "\n",
    "  n = 2500\n",
    "  K = 100\n",
    "  lr = 0.001\n",
    "  for it in tqdm(range(n)):\n",
    "    key, *subs = random.split(key, K + 1)\n",
    "    qz_params = qz_params + lr * grad_elbo(train_data, qz_params,\n",
    "                                           params['dec'], np.stack(subs))\n",
    "    if it == 0 or (it + 1) % 100 == 0:\n",
    "      loss_ = elbo_half(train_data, qz_params, params['dec'], np.stack(subs))\n",
    "      tqdm.write(f\"Iteration {it} \\t | \\t ELBO {loss_:.3f}\")\n",
    "\n",
    "  return qz_params\n",
    "\n",
    "\n",
    "def joint_isocountors(params, qz_params, train_data):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    params: variational (encoder) and generator network parameters\n",
    "    qz_params: approximate posterior optimizer parameters\n",
    "\n",
    "  Plot isocontours of joint distribution p(z, top half of data x) and \n",
    "  optimized approximate posterior q_phi (z | top half of data x).\n",
    "  \"\"\"\n",
    "\n",
    "  def plt_isocontours(ax,\n",
    "                      fn,\n",
    "                      xlim=[-6, 6],\n",
    "                      ylim=[-6, 6],\n",
    "                      numticks=101,\n",
    "                      colors=None,\n",
    "                      levels=10):\n",
    "    \"\"\"Plot isocountours of distributions.\"\"\"\n",
    "    x = onp.linspace(*xlim, num=numticks)\n",
    "    y = onp.linspace(*ylim, num=numticks)\n",
    "    X, Y = onp.meshgrid(x, y)\n",
    "    inputs = onp.concatenate(\n",
    "        [onp.atleast_2d(X.ravel()),\n",
    "         onp.atleast_2d(Y.ravel())])\n",
    "    zs = onp.array(fn(inputs.T))\n",
    "    Z = zs.reshape(X.shape)\n",
    "    cs = plt.contour(X, Y, Z, colors=colors, levels=levels)\n",
    "    plt.clabel(cs, inline=1, fontsize=10, fmt='%.2g')\n",
    "\n",
    "  fig = plt.figure(figsize=(8, 8), facecolor='white')\n",
    "  ax = fig.add_subplot(111, frameon=False)\n",
    "  plt_isocontours(\n",
    "      ax,\n",
    "      lambda z: vmap(joint_ll_top_half, in_axes=(None, 0, None))\n",
    "      (train_data, z, params['dec']),\n",
    "      colors='g')\n",
    "  plt_isocontours(\n",
    "      ax,\n",
    "      lambda z: vmap(diag_gaussian_log_density, in_axes=(0, None, None))\n",
    "      (z, *unpack_gaussian_params(qz_params)),\n",
    "      colors='b')\n",
    "  plt.grid()\n",
    "  plt.xlabel(r\"$z_0$\")\n",
    "  plt.ylabel(r\"$z_1$\")\n",
    "  lines = [\n",
    "      mpl.lines.Line2D([0], [0], color='g'),\n",
    "      mpl.lines.Line2D([0], [0], color='b')\n",
    "  ]\n",
    "  plt.title(r'Isocountours of $\\log p$ and $\\log q$ posteriors')\n",
    "  ax.legend(lines, ['true log posterior p', 'variational log posterior q'])\n",
    "  plt.tight_layout(rect=(0, 0, 1, 1))\n",
    "  plt.savefig('isocountours.png')\n",
    "\n",
    "\n",
    "# def infer_bottom_half(params, qz_params, train_data, seed=412):\n",
    "#   \"\"\"\n",
    "#   Args:\n",
    "#     params: decoder\n",
    "#     qz_params: variational optimized posterior params\n",
    "#     train_data: single digit trained on\n",
    "\n",
    "#   Plots original whole data beside inferred greyscale.\n",
    "#   \"\"\"\n",
    "#   key = random.PRNGKey(seed)\n",
    "#   key, subkey = random.split(key)\n",
    "#   # sample z ~ approximate posterior q, feed it to decoder to find\n",
    "#   # Bernoulli means of p(bottom half of data | x).\n",
    "#   z = sample_diag_gaussian(*unpack_gaussian_params(qz_params), subkey)\n",
    "#   x = sigmoid(decoder(z, params['dec']))\n",
    "\n",
    "#   data_ = onp.zeros((28, 28))\n",
    "#   data_[:14, :] = train_data.reshape([28, 28])[:14, :]  # original top half\n",
    "#   data_[14:, :] = x.reshape([28, 28])[14:, :]  # inferred bottom half\n",
    "\n",
    "#   plt_im = onp.zeros((28, 28 * 2))\n",
    "#   plt_im[:, :28] = data_\n",
    "#   plt_im[:, 28:] = train_data.reshape([28, 28])\n",
    "\n",
    "#   fig, ax = plt.subplots()\n",
    "#   plt.imshow(plt_im, cmap=plt.cm.binary)\n",
    "#   plt.axis('off')\n",
    "#   plt.savefig('frankenstein_bottom_to_top.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  train_data, test_data = train_input, test_input\n",
    "\n",
    "  # change the seed\n",
    "  seed = 412\n",
    "  num_samples = 10\n",
    "  train(train_data, test_data, 'params.pkl', seed)\n",
    "\n",
    "  # plot samples form generative model\n",
    "  opt_params = load_params('params.pkl')\n",
    "  sample_gen(opt_params, num_samples, seed)\n",
    "  latent_means(opt_params, train_data, train_labels)\n",
    "  interpolate_ex = [(1, 2), (3, 8), (4, 5)]\n",
    "  lin_interpolate(opt_params, train_data, train_labels, interpolate_ex)\n",
    "\n",
    "  # non-amortized inference (we are selecting one good sample)\n",
    "  select_im_good = train_data[1]\n",
    "  qz_params = optimize_params(opt_params, select_im_good, seed)\n",
    "  joint_isocountors(opt_params, qz_params, select_im_good)\n",
    "  infer_bottom_half(opt_params, qz_params, select_im_good)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glue_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c204853792b21fcac3f33003e8cf0635bb37268bd89f72590d4a7ab30dd59ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
